{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Reset every staging tier except for Landing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!aws s3 rm s3://test-lf-wm/etl/raw --recursive \n",
                "!aws s3 rm s3://test-lf-wm/etl/access --recursive \n",
                "!aws s3 rm s3://test-lf-wm/etl/optimised --recursive "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load source data into `Bucket.TEST` and `Bucket.PROD` if needed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2023-06-03 00:21:47      22655 etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "2023-06-03 00:21:51       5818 etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "2023-06-03 00:21:55        814 etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 ls s3://project-lf/etl/landing --recursive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "upload: ../mock/source_files/202305211851-claim-full.csv to s3://test-lf-ap/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "upload: ../mock/source_files/202305221132-policyholder-full.csv to s3://test-lf-ap/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "upload: ../mock/source_files/202305221136-provider-full.csv to s3://test-lf-ap/etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/mock/source_files/202305211851-claim-full.csv s3://test-lf-ap/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                "\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/mock/source_files/202305221132-policyholder-full.csv s3://test-lf-ap/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                "\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/mock/source_files/202305221136-provider-full.csv s3://test-lf-ap/etl/landing/claim_db/provider/full/202305221136-provider-full.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "test-lf-wm etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "test-lf-wm etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "test-lf-wm etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "env = Bucket.TEST\n",
                "\n",
                "files = {\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/mock/source_files/202305211851-claim-full.csv\": f\"s3://{env}/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\",\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/mock/source_files/202305221132-policyholder-full.csv\": f\"s3://{env}/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\",\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/mock/source_files/202305221136-provider-full.csv\": f\"s3://{env}/etl/landing/claim_db/provider/full/202305221136-provider-full.csv\",\n",
                "}\n",
                "\n",
                "# s3_resource = S3Resource.getInstance()\n",
                "\n",
                "for src, dest in files.items():\n",
                "\n",
                "    print(bucket_name, key)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Delete every glue job"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# delete all glue ETL jobs\n",
                "glue = boto3.client('glue', region_name='ap-southeast-2')\n",
                "jobs = glue.get_jobs()\n",
                "for job in jobs['Jobs']:\n",
                "    if job['Name'].startswith('stage_'):\n",
                "        glue.delete_job(JobName=job['Name'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refresh every .py job file in `code/jobs/` and `code/dependencies` "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "delete: s3://test-project-wm/code/batches/batch2.py\n",
                        "delete: s3://test-project-wm/code/batches/batch1.py\n",
                        "delete: s3://test-project-wm/code/batches/batch3.py\n",
                        "delete: s3://test-project-wm/code/batches/batch4.py\n",
                        "delete: s3://test-project-wm/code/batches/batch5.py\n"
                    ]
                }
            ],
            "source": [
                "# delete the libraries.zip file at s3://test-project-wm/code/libraries.zip\n",
                "!aws s3 rm s3://test-project-wm/code/libraries.zip\n",
                "\n",
                "# write cli that copies libraries.zip to s3://test-project-wm/code/libraries.zip\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/libraries.zip s3://test-project-wm/code/dependencies/libraries.zip\n",
                "\n",
                "# delete everything but the libraries.zip file at s3://test-project-wm/code/\n",
                "!aws s3 rm s3://test-project-wm/code/batches --recursive"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload .py batch files to S3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Uploaded 5 files to s3://test-project-wm/code/batches/\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "# Create an S3 client\n",
                "s3_client = boto3.client('s3', region_name='ap-southeast-1')\n",
                "\n",
                "# Set the bucket and prefix\n",
                "bucket = 'test-project-wm'\n",
                "prefix = 'code/batches/'\n",
                "\n",
                "# Set the list of files to upload\n",
                "files = [\n",
                "    '/home/glue_user/project_lf/ETL-TDD/batch1.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/batch2.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/batch3.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/batch4.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/batch5.py',\n",
                "]\n",
                "\n",
                "# Upload each file to the S3 bucket\n",
                "for file_path in files:\n",
                "    # Get the file name\n",
                "    file_name = file_path.split('/')[-1]\n",
                "\n",
                "    # Set the object key\n",
                "    key = f'{prefix}{file_name}'\n",
                "\n",
                "    # Upload the file\n",
                "    s3_client.upload_file(file_path, bucket, key)\n",
                "\n",
                "print(f'Uploaded {len(files)} files to s3://{bucket}/{prefix}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialise Glue session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for each script in s3://project-lf/code/jobs/ create a glue job\n",
                "import boto3\n",
                "\n",
                "# Create an AWS Glue client\n",
                "glue_client = boto3.client('glue', region_name='ap-southeat-2')\n",
                "\n",
                "# Set the parameters for the new Glue job\n",
                "glue_role = 'data-quality-lf'\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get a list of all script objects in `project-lf/code/jobs/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[s3.ObjectSummary(bucket_name='test-project-wm', key='code/batches/batch1.py'),\n",
                            " s3.ObjectSummary(bucket_name='test-project-wm', key='code/batches/batch2.py'),\n",
                            " s3.ObjectSummary(bucket_name='test-project-wm', key='code/batches/batch3.py'),\n",
                            " s3.ObjectSummary(bucket_name='test-project-wm', key='code/batches/batch4.py'),\n",
                            " s3.ObjectSummary(bucket_name='test-project-wm', key='code/batches/batch5.py')]"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# get all keys in s3://project-lf/code/jobs/\n",
                "s3_resource = boto3.resource('s3')\n",
                "bucket = 'test-project-wm'\n",
                "prefix = 'code/batches/'\n",
                "objects = s3_resource.Bucket(bucket).objects.filter(Prefix=prefix)\n",
                "list(objects)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create a job for every script object"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "code/batches/batch1.py\n",
                        "{'Name': 'batch1', 'ResponseMetadata': {'RequestId': '98aca703-2c52-4a92-a51f-3eaadfe4ef46', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Jun 2023 05:59:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '17', 'connection': 'keep-alive', 'x-amzn-requestid': '98aca703-2c52-4a92-a51f-3eaadfe4ef46'}, 'RetryAttempts': 0}}\n",
                        "code/batches/batch2.py\n",
                        "{'Name': 'batch2', 'ResponseMetadata': {'RequestId': 'cbb22ab5-7c38-4557-a9d1-638d69331d78', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Jun 2023 05:59:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '17', 'connection': 'keep-alive', 'x-amzn-requestid': 'cbb22ab5-7c38-4557-a9d1-638d69331d78'}, 'RetryAttempts': 0}}\n",
                        "code/batches/batch3.py\n",
                        "{'Name': 'batch3', 'ResponseMetadata': {'RequestId': '5474a1bd-1c20-402e-9e0e-df50ec933256', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Jun 2023 05:59:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '17', 'connection': 'keep-alive', 'x-amzn-requestid': '5474a1bd-1c20-402e-9e0e-df50ec933256'}, 'RetryAttempts': 0}}\n",
                        "code/batches/batch4.py\n",
                        "{'Name': 'batch4', 'ResponseMetadata': {'RequestId': '20f245e2-afe9-4b17-907f-bdc40c4fbe4c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Jun 2023 05:59:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '17', 'connection': 'keep-alive', 'x-amzn-requestid': '20f245e2-afe9-4b17-907f-bdc40c4fbe4c'}, 'RetryAttempts': 0}}\n",
                        "code/batches/batch5.py\n",
                        "{'Name': 'batch5', 'ResponseMetadata': {'RequestId': 'a44eb66d-eb6f-4679-b465-e6844ed9bf4a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Jun 2023 05:59:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '17', 'connection': 'keep-alive', 'x-amzn-requestid': 'a44eb66d-eb6f-4679-b465-e6844ed9bf4a'}, 'RetryAttempts': 0}}\n"
                    ]
                }
            ],
            "source": [
                "glue_client = boto3.client('glue', region_name='ap-southeast-2')\n",
                "job_names = []\n",
                "for s3_object in objects:\n",
                "    print(s3_object.key)\n",
                "    \n",
                "    # Create an AWS Glue client\n",
                "\n",
                "    # Set the parameters for the new Glue job\n",
                "    glue_role = 'data-quality-lf'\n",
                "    script_location = f's3://test-project-wm/{s3_object.key}'\n",
                "    job_name = script_location.split('/')[-1].split('.')[0]\n",
                "    extra_py_files = 's3://test-project-wm/code/dependencies/libraries.zip'\n",
                "    glue_version = '4.0'\n",
                "    additional_python_modules = \"holidays\"\n",
                "\n",
                "    try:\n",
                "        # Create the new Glue job\n",
                "        response = glue_client.create_job(\n",
                "            Name=job_name,\n",
                "            Role=glue_role,\n",
                "            Command={'Name': 'glueetl', 'ScriptLocation': script_location},\n",
                "            GlueVersion=glue_version,\n",
                "            DefaultArguments={\n",
                "                '--extra-py-files': extra_py_files,\n",
                "                '--additional-python-modules': additional_python_modules\n",
                "            },\n",
                "            WorkerType='G.1X',\n",
                "            NumberOfWorkers=2\n",
                "        )\n",
                "        print(response)\n",
                "    except Exception as e:\n",
                "        print(\"Skipping job creation for\", job_name, e)\n",
                "    \n",
                "    job_names.append(job_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define execution order of jobs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "batches = [\n",
                "     'batch1', 'batch2', 'batch3', 'batch4', 'batch5'\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define a function to run (in parallel) all jobs in a list and wait until they have completed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "import time\n",
                "import colorama\n",
                "from colorama import Fore, Style\n",
                "colorama.init()\n",
                "\n",
                "# Create an AWS Glue client\n",
                "glue_client = boto3.client('glue', region_name='ap-southeast-2')\n",
                "\n",
                "# Function to run a list of batches and wait for them to finish\n",
                "def run_batches(batches):\n",
                "    # Start each job and store its job run ID\n",
                "    job_run_ids = {}\n",
                "    for batch_name in batches:\n",
                "        response = glue_client.start_job_run(JobName=batch_name)\n",
                "        job_run_id = response['JobRunId']\n",
                "        job_run_ids[batch_name] = job_run_id\n",
                "        print(batch_name, job_run_id)\n",
                "    \n",
                "    # Wait for all batches to finish\n",
                "    while True:\n",
                "        # Check the status of each job run\n",
                "        all_finished = True\n",
                "    \n",
                "        for job_name, job_run_id in job_run_ids.items():\n",
                "            try:\n",
                "                response = glue_client.get_job(JobName=job_name)\n",
                "                response = glue_client.get_job_run(JobName=job_name, RunId=job_run_id)\n",
                "                status = response['JobRun']['JobRunState']\n",
                "                if status not in ['SUCCEEDED', 'FAILED', 'STOPPED']:\n",
                "                    all_finished = False\n",
                "                else:\n",
                "                    print(\".\", end=\"\")\n",
                "            except glue_client.exceptions.EntityNotFoundException:\n",
                "                continue\n",
                "        print(\"-\"*80)\n",
                "        # If all batches have finished, exit the loop\n",
                "        if all_finished:\n",
                "            print(\"Finished.\")\n",
                "        # Otherwise, wait and check again\n",
                "        time.sleep(1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execute jobs in order"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_batches(batches)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Troubleshooting DFs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SLF4J: Class path contains multiple SLF4J bindings.\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
                        "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
                        "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
                        "log4j:WARN Please initialize the log4j system properly.\n",
                        "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"test\") \\\n",
                "    .getOrCreate()\n",
                "    \n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### provider_dim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- claim_id: integer (nullable = true)\n",
                        " |-- provider_key: long (nullable = true)\n",
                        " |-- policyholder_key: long (nullable = true)\n",
                        " |-- procedure_key: long (nullable = true)\n",
                        " |-- date_key: integer (nullable = true)\n",
                        " |-- total_procedure_cost: float (nullable = true)\n",
                        " |-- medibank_pays: float (nullable = true)\n",
                        " |-- medicare_pays: float (nullable = true)\n",
                        " |-- excess: float (nullable = true)\n",
                        " |-- out_of_pocket: float (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df = spark.read.parquet('s3://test-project-wm/etl/optimised/claim_fact/full/202306072017/')\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------------+--------------------+--------------------+--------+--------------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
                        "|location_key|             address|              street|postcode|              suburb|track_hash|    record_start_ts|      record_end_ts|record_active_flag|       record_upd_ts|    record_insert_ts|\n",
                        "+------------+--------------------+--------------------+--------+--------------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
                        "|           0|Apt. 486 646 Mich...|Apt. 486 646 Mich...|    2901| Lake Gabrielchester|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-07 10:08:...|2023-06-07 10:08:...|\n",
                        "|           1|Level 2 85 Rhonda...|Level 2 85 Rhonda...|    2987|           Jimmyfurt|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-07 10:08:...|2023-06-07 10:08:...|\n",
                        "|           2|Flat 00 575 Erica...|Flat 00 575 Erica...|    9818|       Elizabethbury|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-07 10:08:...|2023-06-07 10:08:...|\n",
                        "|           3|695 Ryan Nook, St...|       695 Ryan Nook|    2697|         St. Patrick|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-07 10:08:...|2023-06-07 10:08:...|\n",
                        "|           4|606 Hamilton Circ...| 606 Hamilton Circle|    2957|          Munozburgh|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-07 10:08:...|2023-06-07 10:08:...|\n",
                        "+------------+--------------------+--------------------+--------+--------------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
                        "only showing top 5 rows\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DataFrame[procedure: bigint, procedure_key: bigint, track_hash: bigint, record_start_ts: string, record_end_ts: string, record_active_flag: smallint, record_upd_ts: string, record_insert_ts: string]"
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from pyspark.sql.functions import col\n",
                "df = df.select(\n",
                "    col(\"procedure\").cast(\"long\"),\n",
                "    col(\"procedure_key\").cast(\"long\"),\n",
                "    col(\"track_hash\").cast(\"long\"),\n",
                "    col(\"record_start_ts\").cast(\"string\"),\n",
                "    col(\"record_end_ts\").cast(\"string\"),\n",
                "    col(\"record_active_flag\").cast(\"short\"),\n",
                "    col(\"record_upd_ts\").cast(\"string\"),\n",
                "    col(\"record_insert_ts\").cast(\"string\")\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = spark.read.parquet(\"s3://test-project-wm/etl/optimised/provider_dim/full/202306072013/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- provider_key: long (nullable = true)\n",
                        " |-- location_key: long (nullable = true)\n",
                        " |-- provider_license_number: string (nullable = true)\n",
                        " |-- provider_id: integer (nullable = true)\n",
                        " |-- provider_name: string (nullable = true)\n",
                        " |-- provider_phone_number: string (nullable = true)\n",
                        " |-- provider_email_address: string (nullable = true)\n",
                        " |-- provider_type: string (nullable = true)\n",
                        " |-- track_hash: long (nullable = true)\n",
                        " |-- record_start_ts: string (nullable = true)\n",
                        " |-- record_end_ts: string (nullable = true)\n",
                        " |-- record_active_flag: short (nullable = true)\n",
                        " |-- record_upd_ts: string (nullable = true)\n",
                        " |-- record_insert_ts: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.types import StructType, StructField, StringType, LongType, ShortType\n",
                "\n",
                "schema = StructType([StructField('procedure', LongType(), True), StructField('procedure_key', LongType(), True), StructField('track_hash', LongType(), True), StructField('record_start_ts', StringType(), True), StructField('record_end_ts', StringType(), True), StructField('record_active_flag', ShortType(), True), StructField('record_upd_ts', StringType(), True), StructField('record_insert_ts', StringType(), True)])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Reset every staging tier except for Landing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!aws s3 rm s3://test-lf-wm/etl/raw --recursive \n",
                "!aws s3 rm s3://test-lf-wm/etl/access --recursive \n",
                "!aws s3 rm s3://test-lf-wm/etl/optimised --recursive "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load source data into `Bucket.TEST` and `Bucket.PROD` if needed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2023-06-03 00:21:47      22655 etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "2023-06-03 00:21:51       5818 etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "2023-06-03 00:21:55        814 etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 ls s3://project-lf/etl/landing --recursive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "upload: ../etl/mock/source_files/202305211851-claim-full.csv to s3://test-lf-ap/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "upload: ../etl/mock/source_files/202305221132-policyholder-full.csv to s3://test-lf-ap/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "upload: ../etl/mock/source_files/202305221136-provider-full.csv to s3://test-lf-ap/etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305211851-claim-full.csv s3://test-lf-ap/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                "\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221132-policyholder-full.csv s3://test-lf-ap/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                "\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221136-provider-full.csv s3://test-lf-ap/etl/landing/claim_db/provider/full/202305221136-provider-full.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "test-lf-wm etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "test-lf-wm etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "test-lf-wm etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "env = Bucket.TEST\n",
                "\n",
                "files = {\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305211851-claim-full.csv\": f\"s3://{env}/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\",\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221132-policyholder-full.csv\": f\"s3://{env}/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\",\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221136-provider-full.csv\": f\"s3://{env}/etl/landing/claim_db/provider/full/202305221136-provider-full.csv\",\n",
                "}\n",
                "\n",
                "# s3_resource = S3Resource.getInstance()\n",
                "\n",
                "for src, dest in files.items():\n",
                "\n",
                "    print(bucket_name, key)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Delete every glue job"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [],
            "source": [
                "# delete all glue ETL jobs\n",
                "glue = boto3.client('glue', region_name='ap-southeast-2')\n",
                "jobs = glue.get_jobs()\n",
                "for job in jobs['Jobs']:\n",
                "    if job['Name'].startswith('stage_'):\n",
                "        glue.delete_job(JobName=job['Name'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refresh every .py job file in `code/jobs/` and `code/dependencies` "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "delete: s3://test-project-wm/code/libraries.zip\n",
                        "updating: batch.py (deflated 57%)\n",
                        "updating: CICD/ (stored 0%)\n",
                        "updating: databrew.py (deflated 74%)\n",
                        "updating: docker.md (deflated 24%)\n",
                        "updating: glue_databrew.ipynb (deflated 88%)\n",
                        "updating: glue.py (deflated 79%)\n",
                        "updating: __init__.py (stored 0%)\n",
                        "updating: job_orchestration.ipynb (deflated 86%)\n",
                        "updating: __pycache__/ (stored 0%)\n",
                        "updating: __pycache__/glue_job.cpython-310.pyc (deflated 66%)\n",
                        "updating: __pycache__/redshift.cpython-310.pyc (deflated 46%)\n",
                        "updating: __pycache__/__init__.cpython-310.pyc (deflated 22%)\n",
                        "updating: __pycache__/databrew.cpython-310.pyc (deflated 54%)\n",
                        "updating: __pycache__/glue.cpython-310.pyc (deflated 62%)\n",
                        "updating: __pycache__/batch.cpython-310.pyc (deflated 40%)\n",
                        "updating: readme.md (deflated 55%)\n",
                        "updating: redshift.ipynb (deflated 85%)\n",
                        "updating: redshift.py (deflated 57%)\n",
                        "updating: runs/ (stored 0%)\n",
                        "updating: runs/run1_provider.py (deflated 59%)\n",
                        "updating: runs/run1_date.py (deflated 55%)\n",
                        "updating: runs/run3_policyholder.py (deflated 52%)\n",
                        "updating: runs/run1_policyholder.py (deflated 60%)\n",
                        "updating: runs/run4_claim_fact.py (deflated 52%)\n",
                        "updating: runs/run3_provider.py (deflated 53%)\n",
                        "updating: runs/run1_claim.py (deflated 71%)\n",
                        "updating: runs/run2_location.py (deflated 52%)\n",
                        "updating: runs/run2_procedure.py (deflated 52%)\n",
                        "updating: runs/__pycache__/ (stored 0%)\n",
                        "updating: runs/__pycache__/run1_provider.cpython-310.pyc (deflated 38%)\n",
                        "updating: runs/__pycache__/run1_date.cpython-310.pyc (deflated 37%)\n",
                        "updating: runs/__pycache__/run1_policyholder.cpython-310.pyc (deflated 37%)\n",
                        "updating: runs/__pycache__/run1_claim.cpython-310.pyc (deflated 38%)\n",
                        "updating: shell_utilities/ (stored 0%)\n",
                        "updating: shell_utilities/file_renaming.sh (deflated 70%)\n",
                        "updating: shell_utilities/simple_end_to_end.sh (deflated 76%)\n",
                        "upload: ./libraries.zip to s3://test-project-wm/code/dependencies/libraries.zip\n"
                    ]
                }
            ],
            "source": [
                "# delete the libraries.zip file at s3://test-project-wm/code/libraries.zip\n",
                "!aws s3 rm s3://test-project-wm/code/libraries.zip\n",
                "\n",
                "# create a new zip file\n",
                "!zip -r libraries.zip *\n",
                "\n",
                "# write cli that copies libraries.zip to s3://test-project-wm/code/libraries.zip\n",
                "!aws s3 cp libraries.zip s3://test-project-wm/code/dependencies/libraries.zip\n",
                "\n",
                "# delete everything but the libraries.zip file at s3://test-project-wm/code/\n",
                "# !aws s3 rm s3://test-project-wm/code/ --recursive --exclude \"s3://test-project-wm/code/dependencies/libraries.zip\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload .py batch files to S3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Uploaded 1 files to s3://test-project-wm/code/jobs/\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "# Create an S3 client\n",
                "s3_client = boto3.client('s3', region_name='ap-southeast-1')\n",
                "\n",
                "# Set the bucket and prefix\n",
                "bucket = 'test-project-wm'\n",
                "prefix = 'code/jobs/'\n",
                "\n",
                "# Set the list of files to upload\n",
                "files = [\n",
                "    '/home/glue_user/project_lf/ETL-TDD/batch1.py',\n",
                "]\n",
                "\n",
                "# Upload each file to the S3 bucket\n",
                "for file_path in files:\n",
                "    # Get the file name\n",
                "    file_name = file_path.split('/')[-1]\n",
                "\n",
                "    # Set the object key\n",
                "    key = f'{prefix}{file_name}'\n",
                "\n",
                "    # Upload the file\n",
                "    s3_client.upload_file(file_path, bucket, key)\n",
                "\n",
                "print(f'Uploaded {len(files)} files to s3://{bucket}/{prefix}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialise Glue session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for each script in s3://project-lf/code/jobs/ create a glue job\n",
                "import boto3\n",
                "\n",
                "# Create an AWS Glue client\n",
                "glue_client = boto3.client('glue', region_name='ap-southeat-2')\n",
                "\n",
                "# Set the parameters for the new Glue job\n",
                "glue_role = 'data-quality-lf'\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get a list of all script objects in `project-lf/code/jobs/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[s3.ObjectSummary(bucket_name='test-project-wm', key='code/jobs/batch1.py')]"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# get all keys in s3://project-lf/code/jobs/\n",
                "s3_resource = boto3.resource('s3')\n",
                "bucket = 'test-project-wm'\n",
                "prefix = 'code/jobs/'\n",
                "objects = s3_resource.Bucket(bucket).objects.filter(Prefix=prefix)\n",
                "list(objects)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create a job for every script object"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "code/jobs/batch1.py\n",
                        "{'Name': 'batch1', 'ResponseMetadata': {'RequestId': 'd363225e-98e0-412a-bb70-dfd1ae3efce7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 06 Jun 2023 11:52:53 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '17', 'connection': 'keep-alive', 'x-amzn-requestid': 'd363225e-98e0-412a-bb70-dfd1ae3efce7'}, 'RetryAttempts': 0}}\n"
                    ]
                }
            ],
            "source": [
                "glue_client = boto3.client('glue', region_name='ap-southeast-2')\n",
                "job_names = []\n",
                "for s3_object in objects:\n",
                "    print(s3_object.key)\n",
                "    \n",
                "    # Create an AWS Glue client\n",
                "\n",
                "    # Set the parameters for the new Glue job\n",
                "    glue_role = 'data-quality-lf'\n",
                "    script_location = f's3://test-project-wm/{s3_object.key}'\n",
                "    job_name = script_location.split('/')[-1].split('.')[0]\n",
                "    extra_py_files = 's3://test-project-wm/code/dependencies/libraries.zip'\n",
                "    glue_version = '4.0'\n",
                "    additional_python_modules = \"holidays\"\n",
                "\n",
                "    try:\n",
                "        # Create the new Glue job\n",
                "        response = glue_client.create_job(\n",
                "            Name=job_name,\n",
                "            Role=glue_role,\n",
                "            Command={'Name': 'glueetl', 'ScriptLocation': script_location},\n",
                "            GlueVersion=glue_version,\n",
                "            DefaultArguments={\n",
                "                '--extra-py-files': extra_py_files,\n",
                "                '--additional-python-modules': additional_python_modules\n",
                "            },\n",
                "            WorkerType='G.1X',\n",
                "            NumberOfWorkers=2\n",
                "        )\n",
                "        print(response)\n",
                "    except Exception as e:\n",
                "        print(\"Skipping job creation for\", job_name, e)\n",
                "    \n",
                "    job_names.append(job_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define execution order of jobs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "batches = [\n",
                "     'batch1'\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define a function to run (in parallel) all jobs in a list and wait until they have completed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "import time\n",
                "import colorama\n",
                "from colorama import Fore, Style\n",
                "colorama.init()\n",
                "\n",
                "# Create an AWS Glue client\n",
                "glue_client = boto3.client('glue', region_name='us-east-1')\n",
                "\n",
                "# Function to run a list of batches and wait for them to finish\n",
                "def run_batches(batches):\n",
                "    # Start each job and store its job run ID\n",
                "    job_run_ids = {}\n",
                "    for batch_name in batches:\n",
                "        response = glue_client.start_job_run(JobName=batch_name)\n",
                "        job_run_id = response['JobRunId']\n",
                "        job_run_ids[batch_name] = job_run_id\n",
                "        print(batch_name, job_run_id)\n",
                "\n",
                "    print(\"-\"*80)\n",
                "    \n",
                "    # Wait for all batches to finish\n",
                "    while True:\n",
                "        # Check the status of each job run\n",
                "        all_finished = True\n",
                "    \n",
                "        for job_name, job_run_id in job_run_ids.items():\n",
                "            try:\n",
                "                response = glue_client.get_job(JobName=job_name)\n",
                "                response = glue_client.get_job_run(JobName=job_name, RunId=job_run_id)\n",
                "                status = response['JobRun']['JobRunState']\n",
                "                if status not in ['SUCCEEDED', 'FAILED', 'STOPPED']:\n",
                "                    all_finished = False\n",
                "                else:\n",
                "                    if status == 'SUCCEEDED':\n",
                "                        print(Fore.GREEN + '\\t' + job_name + ' has ' + status + '.' + Style.RESET_ALL)\n",
                "                    elif status == 'FAILED':\n",
                "                        error_message = response['JobRun'].get('ErrorMessage', 'No error message available')\n",
                "                        print(Fore.RED + '\\t' + job_name + ' has ' + status + '.' + Style.RESET_ALL)\n",
                "                        print(Fore.RED + f'\\t{job_name} has {status}. Error message: {error_message}' + Style.RESET_ALL)\n",
                "                    else:\n",
                "                        print(Fore.YELLOW + f'\\t{job_name} has {status}.' + Style.RESET_ALL)\n",
                "            except glue_client.exceptions.EntityNotFoundException:\n",
                "                continue\n",
                "        print(\"-\"*80)\n",
                "        # If all batches have finished, exit the loop\n",
                "        if all_finished:\n",
                "            break\n",
                "        # Otherwise, wait and check again\n",
                "        time.sleep(10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execute jobs in order"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "EntityNotFoundException",
                    "evalue": "An error occurred (EntityNotFoundException) when calling the StartJobRun operation: Failed to start job run due to missing metadata",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mEntityNotFoundException\u001b[0m                   Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_batches(batches)\n",
                        "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mrun_batches\u001b[0;34m(batches)\u001b[0m\n\u001b[1;32m     13\u001b[0m job_run_ids \u001b[39m=\u001b[39m {}\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m batch_name \u001b[39min\u001b[39;00m batches:\n\u001b[0;32m---> 15\u001b[0m     response \u001b[39m=\u001b[39m glue_client\u001b[39m.\u001b[39;49mstart_job_run(JobName\u001b[39m=\u001b[39;49mbatch_name)\n\u001b[1;32m     16\u001b[0m     job_run_id \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mJobRunId\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m     job_run_ids[batch_name] \u001b[39m=\u001b[39m job_run_id\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:514\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:938\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
                        "\u001b[0;31mEntityNotFoundException\u001b[0m: An error occurred (EntityNotFoundException) when calling the StartJobRun operation: Failed to start job run due to missing metadata"
                    ]
                }
            ],
            "source": [
                "run_batches(batches)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "********************************************************************************\n",
                        "Starting on Run 1: ['stage_claim_into_raw', 'stage_provider_into_raw', 'stage_policyholder_into_raw', 'stage_date_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_claim_into_raw jr_f013cdcd5ba3156e87729f26f19dc25d5ebcffcf263ab4858c02f2b56e4d5776\n",
                        "stage_provider_into_raw jr_06ab19d466f1cb9a05dd8fe770393be13bebf127b9434129619e81282a2d5166\n",
                        "stage_policyholder_into_raw jr_fdc34733a113a4eaba4bb9a66582be5bf7208a1807792cd3f7bcc72d1b389858\n",
                        "stage_date_into_optimised jr_0a425aa1c16043e5e8f600a973c86f7a4d4890574cd22b8b1a073d1428696990\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_provider_into_rawhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_claim_into_rawhas SUCCEEDED.\n",
                        "\tstage_provider_into_rawhas SUCCEEDED.\n",
                        "\tstage_policyholder_into_rawhas SUCCEEDED.\n",
                        "\tstage_date_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 2: ['stage_claim_into_access', 'stage_provider_into_access', 'stage_policyholder_into_access']\n",
                        "********************************************************************************\n",
                        "stage_claim_into_access jr_d13a12e9e73dcd8f3d165860b44f0ef7b384e14fe0c28f542098e5dae4f8a6ca\n",
                        "stage_provider_into_access jr_f9525163d5adcce6169f188a173c3431125844e6f8a646671fa85d34eefa0a1b\n",
                        "stage_policyholder_into_access jr_74ca915757abcd9ea065bccd4e702a3dfbfc05d933440aac54326c85f8201927\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_provider_into_accesshas SUCCEEDED.\n",
                        "\tstage_policyholder_into_accesshas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_claim_into_accesshas SUCCEEDED.\n",
                        "\tstage_provider_into_accesshas SUCCEEDED.\n",
                        "\tstage_policyholder_into_accesshas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 3: ['stage_location_into_optimised', 'stage_procedure_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_location_into_optimised jr_a5a6881e36bd0f6041043ceff6336e2ad599bcf05abb80ce895e11ea5fb083a9\n",
                        "stage_procedure_into_optimised jr_950089f62361213c7733e66546bd76bdc03e7fbc7ad01a0033873eea62d9c5b1\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_location_into_optimisedhas SUCCEEDED.\n",
                        "\tstage_procedure_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 4: ['stage_policyholder_into_optimised', 'stage_provider_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_policyholder_into_optimised jr_9ada937f0b9b610b1a15c408abee6ac7bea1ea32ced64b5fce767645e4d2c654\n",
                        "stage_provider_into_optimised jr_a1c965cce6124508bcfd00b5d6578a3e953d20adc65894a005afd779da1aab3b\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_policyholder_into_optimisedhas SUCCEEDED.\n",
                        "\tstage_provider_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 5: ['stage_claim_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_claim_into_optimised jr_52541dcad8424000d6d67dc8dccdaebfa593089c70404723707489d773f280f8\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_claim_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "All jobs finished\n"
                    ]
                }
            ],
            "source": [
                "for count, batch in enumerate(batches):\n",
                "    print(\"*\"*80)\n",
                "    print(f\"Starting on Run {count + 1}: {batch}\")\n",
                "    print(\"*\"*80)\n",
                "    run_batches(batch)\n",
                "print('All jobs finished')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment with boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ClientError",
                    "evalue": "An error occurred (AccessDenied) when calling the CopyObject operation: Access Denied",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[11], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m dataset_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39myour-dataset-name\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Stage the file\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m stage_file(bucket_name, file_key, staging_bucket)\n\u001b[1;32m     59\u001b[0m \u001b[39m# Run the Glue crawler\u001b[39;00m\n\u001b[1;32m     60\u001b[0m run_crawler(crawler_name)\n",
                        "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mstage_file\u001b[0;34m(bucket_name, file_key, staging_bucket)\u001b[0m\n\u001b[1;32m      4\u001b[0m s3_client \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39ms3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Copy the file from the source bucket to the staging bucket\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m s3_client\u001b[39m.\u001b[39;49mcopy_object(\n\u001b[1;32m      8\u001b[0m     Bucket\u001b[39m=\u001b[39;49mstaging_bucket,\n\u001b[1;32m      9\u001b[0m     CopySource\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mBucket\u001b[39;49m\u001b[39m'\u001b[39;49m: bucket_name, \u001b[39m'\u001b[39;49m\u001b[39mKey\u001b[39;49m\u001b[39m'\u001b[39;49m: file_key},\n\u001b[1;32m     10\u001b[0m     Key\u001b[39m=\u001b[39;49mfile_key\n\u001b[1;32m     11\u001b[0m )\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:514\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:938\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
                        "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the CopyObject operation: Access Denied"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "def stage_file(bucket_name, file_key, staging_bucket):\n",
                "    s3_client = boto3.client('s3')\n",
                "\n",
                "    # Copy the file from the source bucket to the staging bucket\n",
                "    s3_client.copy_object(\n",
                "        Bucket=staging_bucket,\n",
                "        CopySource={'Bucket': bucket_name, 'Key': file_key},\n",
                "        Key=file_key\n",
                "    )\n",
                "\n",
                "def run_crawler(crawler_name):\n",
                "    glue_client = boto3.client('glue')\n",
                "\n",
                "    # Start the Glue crawler\n",
                "    response = glue_client.start_crawler(Name=crawler_name)\n",
                "\n",
                "    # Wait for the crawler to complete\n",
                "    glue_client.get_waiter('crawler_complete').wait(\n",
                "        Name=crawler_name,\n",
                "        WaiterConfig={'Delay': 2, 'MaxAttempts': 100}\n",
                "    )\n",
                "\n",
                "def run_databrew_profile_job(job_name, dataset_name):\n",
                "    databrew_client = boto3.client('databrew')\n",
                "\n",
                "    # Create a new AWS Glue DataBrew profile job run\n",
                "    response = databrew_client.create_job_run(\n",
                "        Name=job_name,\n",
                "        Type='PROFILE',\n",
                "        DatasetName=dataset_name\n",
                "    )\n",
                "\n",
                "    # Get the job run ID\n",
                "    job_run_id = response['JobRunId']\n",
                "    print(f\"Started DataBrew profile job run. Job Run ID: {job_run_id}\")\n",
                "\n",
                "    # Wait for the job run to complete\n",
                "    databrew_client.get_waiter('job_run_state_change').wait(\n",
                "        Name=job_name,\n",
                "        RunId=job_run_id,\n",
                "        WaiterConfig={'Delay': 10, 'MaxAttempts': 60}\n",
                "    )\n",
                "\n",
                "    print(\"DataBrew profile job run completed.\")\n",
                "\n",
                "# Example usage\n",
                "bucket_name = 'your-source-bucket'\n",
                "file_key = 'path/to/file.csv'\n",
                "staging_bucket = 'your-staging-bucket'\n",
                "crawler_name = 'your-crawler-name'\n",
                "databrew_job_name = 'your-databrew-job-name'\n",
                "dataset_name = 'your-dataset-name'\n",
                "\n",
                "# Stage the file\n",
                "stage_file(bucket_name, file_key, staging_bucket)\n",
                "\n",
                "# Run the Glue crawler\n",
                "run_crawler(crawler_name)\n",
                "\n",
                "# Run the DataBrew profile job on the staged file\n",
                "run_databrew_profile_job(databrew_job_name, dataset_name)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
                "# SPDX-License-Identifier: Apache-2.0\n",
                "\n",
                "\"\"\"\n",
                "Purpose\n",
                "\n",
                "Shows how to use the AWS SDK for Python (Boto3) with AWS Glue to\n",
                "create and manage crawlers, databases, and jobs.\n",
                "\"\"\"\n",
                "\n",
                "import logging\n",
                "from botocore.exceptions import ClientError\n",
                "\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "# snippet-start:[python.example_code.glue.GlueWrapper.full]\n",
                "# snippet-start:[python.example_code.glue.GlueWrapper.decl]\n",
                "class GlueWrapper:\n",
                "    \"\"\"Encapsulates AWS Glue actions.\"\"\"\n",
                "    def __init__(self, glue_client):\n",
                "        \"\"\"\n",
                "        :param glue_client: A Boto3 Glue client.\n",
                "        \"\"\"\n",
                "        self.glue_client = glue_client\n",
                "# snippet-end:[python.example_code.glue.GlueWrapper.decl]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetCrawler]\n",
                "    def get_crawler(self, name):\n",
                "        \"\"\"\n",
                "        Gets information about a crawler.\n",
                "\n",
                "        :param name: The name of the crawler to look up.\n",
                "        :return: Data about the crawler.\n",
                "        \"\"\"\n",
                "        crawler = None\n",
                "        try:\n",
                "            response = self.glue_client.get_crawler(Name=name)\n",
                "            crawler = response['Crawler']\n",
                "        except ClientError as err:\n",
                "            if err.response['Error']['Code'] == 'EntityNotFoundException':\n",
                "                logger.info(\"Crawler %s doesn't exist.\", name)\n",
                "            else:\n",
                "                logger.error(\n",
                "                    \"Couldn't get crawler %s. Here's why: %s: %s\", name,\n",
                "                    err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "                raise\n",
                "        return crawler\n",
                "    # snippet-end:[python.example_code.glue.GetCrawler]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.CreateCrawler]\n",
                "    def create_crawler(self, name, role_arn, db_name, db_prefix, s3_target):\n",
                "        \"\"\"\n",
                "        Creates a crawler that can crawl the specified target and populate a\n",
                "        database in your AWS Glue Data Catalog with metadata that describes the data\n",
                "        in the target.\n",
                "\n",
                "        :param name: The name of the crawler.\n",
                "        :param role_arn: The Amazon Resource Name (ARN) of an AWS Identity and Access\n",
                "                         Management (IAM) role that grants permission to let AWS Glue\n",
                "                         access the resources it needs.\n",
                "        :param db_name: The name to give the database that is created by the crawler.\n",
                "        :param db_prefix: The prefix to give any database tables that are created by\n",
                "                          the crawler.\n",
                "        :param s3_target: The URL to an S3 bucket that contains data that is\n",
                "                          the target of the crawler.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.create_crawler(\n",
                "                Name=name,\n",
                "                Role=role_arn,\n",
                "                DatabaseName=db_name,\n",
                "                TablePrefix=db_prefix,\n",
                "                Targets={'S3Targets': [{'Path': s3_target}]})\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't create crawler. Here's why: %s: %s\",\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.CreateCrawler]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.StartCrawler]\n",
                "    def start_crawler(self, name):\n",
                "        \"\"\"\n",
                "        Starts a crawler. The crawler crawls its configured target and creates\n",
                "        metadata that describes the data it finds in the target data source.\n",
                "\n",
                "        :param name: The name of the crawler to start.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.start_crawler(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't start crawler %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.StartCrawler]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetDatabase]\n",
                "    def get_database(self, name):\n",
                "        \"\"\"\n",
                "        Gets information about a database in your Data Catalog.\n",
                "\n",
                "        :param name: The name of the database to look up.\n",
                "        :return: Information about the database.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_database(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get database %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['Database']\n",
                "    # snippet-end:[python.example_code.glue.GetDatabase]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetTables]\n",
                "    def get_tables(self, db_name):\n",
                "        \"\"\"\n",
                "        Gets a list of tables in a Data Catalog database.\n",
                "\n",
                "        :param db_name: The name of the database to query.\n",
                "        :return: The list of tables in the database.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_tables(DatabaseName=db_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get tables %s. Here's why: %s: %s\", db_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['TableList']\n",
                "    # snippet-end:[python.example_code.glue.GetTables]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.CreateJob]\n",
                "    def create_job(self, name, description, role_arn, script_location):\n",
                "        \"\"\"\n",
                "        Creates a job definition for an extract, transform, and load (ETL) job that can\n",
                "        be run by AWS Glue.\n",
                "\n",
                "        :param name: The name of the job definition.\n",
                "        :param description: The description of the job definition.\n",
                "        :param role_arn: The ARN of an IAM role that grants AWS Glue the permissions\n",
                "                         it requires to run the job.\n",
                "        :param script_location: The Amazon S3 URL of a Python ETL script that is run as\n",
                "                                part of the job. The script defines how the data is\n",
                "                                transformed.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.create_job(\n",
                "                Name=name, Description=description, Role=role_arn,\n",
                "                Command={'Name': 'glueetl', 'ScriptLocation': script_location, 'PythonVersion': '3'},\n",
                "                GlueVersion='3.0')\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't create job %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.CreateJob]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.StartJobRun]\n",
                "    def start_job_run(self, name, input_database, input_table, output_bucket_name):\n",
                "        \"\"\"\n",
                "        Starts a job run. A job run extracts data from the source, transforms it,\n",
                "        and loads it to the output bucket.\n",
                "\n",
                "        :param name: The name of the job definition.\n",
                "        :param input_database: The name of the metadata database that contains tables\n",
                "                               that describe the source data. This is typically created\n",
                "                               by a crawler.\n",
                "        :param input_table: The name of the table in the metadata database that\n",
                "                            describes the source data.\n",
                "        :param output_bucket_name: The S3 bucket where the output is written.\n",
                "        :return: The ID of the job run.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            # The custom Arguments that are passed to this function are used by the\n",
                "            # Python ETL script to determine the location of input and output data.\n",
                "            response = self.glue_client.start_job_run(\n",
                "                JobName=name,\n",
                "                Arguments={\n",
                "                    '--input_database': input_database,\n",
                "                    '--input_table': input_table,\n",
                "                    '--output_bucket_url': f's3://{output_bucket_name}/'})\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't start job run %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobRunId']\n",
                "    # snippet-end:[python.example_code.glue.StartJobRun]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.ListJobs]\n",
                "    def list_jobs(self):\n",
                "        \"\"\"\n",
                "        Lists the names of job definitions in your account.\n",
                "\n",
                "        :return: The list of job definition names.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.list_jobs()\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't list jobs. Here's why: %s: %s\",\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobNames']\n",
                "    # snippet-end:[python.example_code.glue.ListJobs]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetJobRuns]\n",
                "    def get_job_runs(self, job_name):\n",
                "        \"\"\"\n",
                "        Gets information about runs that have been performed for a specific job\n",
                "        definition.\n",
                "\n",
                "        :param job_name: The name of the job definition to look up.\n",
                "        :return: The list of job runs.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_job_runs(JobName=job_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get job runs for %s. Here's why: %s: %s\", job_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobRuns']\n",
                "    # snippet-end:[python.example_code.glue.GetJobRuns]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetJobRun]\n",
                "    def get_job_run(self, name, run_id):\n",
                "        \"\"\"\n",
                "        Gets information about a single job run.\n",
                "\n",
                "        :param name: The name of the job definition for the run.\n",
                "        :param run_id: The ID of the run.\n",
                "        :return: Information about the run.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_job_run(JobName=name, RunId=run_id)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get job run %s/%s. Here's why: %s: %s\", name, run_id,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobRun']\n",
                "    # snippet-end:[python.example_code.glue.GetJobRun]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteJob]\n",
                "    def delete_job(self, job_name):\n",
                "        \"\"\"\n",
                "        Deletes a job definition. This also deletes data about all runs that are\n",
                "        associated with this job definition.\n",
                "\n",
                "        :param job_name: The name of the job definition to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_job(JobName=job_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete job %s. Here's why: %s: %s\", job_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteJob]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteTable]\n",
                "    def delete_table(self, db_name, table_name):\n",
                "        \"\"\"\n",
                "        Deletes a table from a metadata database.\n",
                "\n",
                "        :param db_name: The name of the database that contains the table.\n",
                "        :param table_name: The name of the table to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_table(DatabaseName=db_name, Name=table_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete table %s. Here's why: %s: %s\", table_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteTable]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteDatabase]\n",
                "    def delete_database(self, name):\n",
                "        \"\"\"\n",
                "        Deletes a metadata database from your Data Catalog.\n",
                "\n",
                "        :param name: The name of the database to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_database(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete database %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteDatabase]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteCrawler]\n",
                "    def delete_crawler(self, name):\n",
                "        \"\"\"\n",
                "        Deletes a crawler.\n",
                "\n",
                "        :param name: The name of the crawler to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_crawler(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete crawler %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteCrawler]\n",
                "# snippet-end:[python.example_code.glue.GlueWrapper.full]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Crawler\n",
                "https://docs.aws.amazon.com/glue/latest/dg/example_glue_CreateCrawler_section.html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Couldn't create crawler. Here's why: AlreadyExistsException: 618572314333:boto3-dq-crawler already exists\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "An error occurred (AlreadyExistsException) when calling the CreateCrawler operation: 618572314333:boto3-dq-crawler already exists\n",
                        "Crawler exists.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Couldn't start crawler boto3-dq-crawler. Here's why: CrawlerRunningException: Crawler with name boto3-dq-crawler has already started\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "An error occurred (CrawlerRunningException) when calling the StartCrawler operation: Crawler with name boto3-dq-crawler has already started\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"boto3-dq-crawler\"\n",
                "database_name = \"data-quality\"\n",
                "s3_target_path = \"s3://project-lf/data_quality\"\n",
                "\n",
                "glue = GlueWrapper(boto3.client('glue', region_name='us-east-1'))\n",
                "\n",
                "try:\n",
                "    response = glue.create_crawler(name, role, database_name, '', s3_target_path)\n",
                "    print(response)\n",
                "except glue.glue_client.exceptions.AlreadyExistsException as e:\n",
                "    print(e)\n",
                "    \n",
                "if glue.get_crawler(name) == None:\n",
                "    print(\"Crawler does not exist.\")\n",
                "else:\n",
                "    print(\"Crawler exists.\")\n",
                "    \n",
                "try:\n",
                "    response = glue.start_crawler(name)\n",
                "    print(response)\n",
                "except glue.glue_client.exceptions.CrawlerRunningException as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'Name': 'boto3-dq-crawler', 'Role': 'service-role/AWSGlueServiceRole-ETL', 'Targets': {'S3Targets': [{'Path': 's3://project-lf/data_quality', 'Exclusions': []}], 'JdbcTargets': [], 'MongoDBTargets': [], 'DynamoDBTargets': [], 'CatalogTargets': [], 'DeltaTargets': []}, 'DatabaseName': 'data-quality', 'Classifiers': [], 'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'}, 'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE', 'DeleteBehavior': 'DEPRECATE_IN_DATABASE'}, 'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'}, 'State': 'STOPPING', 'TablePrefix': 'aws_glue_', 'CrawlElapsedTime': 46000, 'CreationTime': datetime.datetime(2023, 6, 4, 0, 51, 11, tzinfo=tzlocal()), 'LastUpdated': datetime.datetime(2023, 6, 4, 0, 51, 11, tzinfo=tzlocal()), 'Version': 1, 'LakeFormationConfiguration': {'UseLakeFormationCredentials': False, 'AccountId': ''}}\n"
                    ]
                }
            ],
            "source": [
                "response = glue.get_crawler(name)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'ResponseMetadata': {'RequestId': '06c3ae2f-bb8b-4c49-bb95-b22564c3fe3d',\n",
                            "  'HTTPStatusCode': 200,\n",
                            "  'HTTPHeaders': {'date': 'Sun, 04 Jun 2023 00:51:47 GMT',\n",
                            "   'content-type': 'application/x-amz-json-1.1',\n",
                            "   'content-length': '2',\n",
                            "   'connection': 'keep-alive',\n",
                            "   'x-amzn-requestid': '06c3ae2f-bb8b-4c49-bb95-b22564c3fe3d'},\n",
                            "  'RetryAttempts': 0}}"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "create_crawler(name, role, database_name, s3_target_path)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create and execute landing tier crawler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"boto3-landing-production\"\n",
                "database_name = \"release-3\"\n",
                "prefix = \"landing\"\n",
                "s3_target_path = \"s3://project-lf/etl/landing\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = GlueWrapper(boto3.client('glue', region_name='us-east-1'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.create_crawler(name, role, database_name, '', s3_target_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.start_crawler(name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": [
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"test-boto3-landing-production\"\n",
                "database_name = \"release-3\"\n",
                "prefix = \"landing\"\n",
                "s3_target_path = \"s3://test-lf-wm/etl/landing\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create and run profile jobs of landing tier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "databrew = boto3.client('databrew', region_name='us-east-1')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ideas"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Wrapper for dq report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "def get_dq_results(bucket, key, s3_resource):\n",
                "    \"\"\"\n",
                "    Get the data quality results from the S3 bucket.\n",
                "    \"\"\"\n",
                "    s3_object = s3_resource.Object(bucket, key)\n",
                "    dq_results = s3_object.get()[\"Body\"].read().decode(\"utf-8\")\n",
                "    dict_results = json.loads(dq_results)\n",
                "\n",
                "    return dict_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bucket = 'project-lf'\n",
                "key = 'dataquality/landing-claimdb-policyholder_45af5c6917a253505482214c95eb38d832948c5aa6c3e80bf4efb71426938ea8.json'\n",
                "\n",
                "s3 = boto3.resource('s3', region_name='us-east-1')\n",
                "get_dq_results(bucket, key, s3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "get_dq_results(bucket, key, s3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ResourceNotFoundException",
                    "evalue": "An error occurred (ResourceNotFoundException) when calling the CreateProfileJob operation: Dataset landing-claimdb-provider wasn't found",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mResourceNotFoundException\u001b[0m                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[99], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m databrew \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39mdatabrew\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[39m=\u001b[39m databrew\u001b[39m.\u001b[39;49mcreate_profile_job(\n\u001b[1;32m      6\u001b[0m     DatasetName\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlanding-claimdb-provider\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     Name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mproduction-landing-claimdb-provider\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     OutputLocation\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      9\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mBucket\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mproject-lf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mKey\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mdataquality/\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m     },\n\u001b[1;32m     12\u001b[0m     RoleArn\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39marn:aws:iam::618572314333:role/DataBrew-Data-Quality-Workflows-DataAccessRole-ZRAZ5RGWCQW0\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:514\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:938\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
                        "\u001b[0;31mResourceNotFoundException\u001b[0m: An error occurred (ResourceNotFoundException) when calling the CreateProfileJob operation: Dataset landing-claimdb-provider wasn't found"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "databrew = boto3.client('databrew')\n",
                "\n",
                "response = databrew.create_profile_job(\n",
                "    DatasetName='landing-claimdb-provider',\n",
                "    Name='production-landing-claimdb-provider',\n",
                "    OutputLocation={\n",
                "        'Bucket': 'project-lf',\n",
                "        'Key': 'dataquality/'\n",
                "    },\n",
                "    RoleArn='arn:aws:iam::618572314333:role/DataBrew-Data-Quality-Workflows-DataAccessRole-ZRAZ5RGWCQW0'\n",
                ")\n",
                "\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "\n",
                "databrew = boto3.client('databrew', region_name='us-east-1')\n",
                "bucket = 'test-lf-wm'\n",
                "key = 'etl/landing/claim_db/provider/full/202305221136-provider-full.csv'\n",
                "dataset_name = 'test-landing-claimdb-provider'\n",
                "\n",
                "response = databrew.create_dataset(\n",
                "    Name=\"202305221132-policyholder-full\",\n",
                "    Format='GLUE_TABLE',\n",
                "    Input={\n",
                "        'DatabaseInputDefinition': {\n",
                "            'DatabaseName': 'test-release-3',\n",
                "            'TableName': dataset_name\n",
                "        }\n",
                "    }\n",
                ") \n",
                "\n",
                "\n",
                "print(response)\n",
                "\n",
                "\n",
                "# ConflictException if dataset already exists."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Start Run 1\n",
                "\n",
                "### Load data.\n",
                "Inputs needed: \n",
                "\n",
                "Process: `stage_x_into_y`\n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Create data catalog.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Connect dataset to data catalog.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Run profile job.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Get profile job results.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Create data quality job.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Run data quality job.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Get data quality job results.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "\n",
                "## Start Run 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "sys.path.append('/home/glue_user/project_lf/ETL-TDD')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {},
            "outputs": [],
            "source": [
                "from automation.databrew import DataBrew\n",
                "\n",
                "databrew = DataBrew('us-east-1')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SLF4J: Class path contains multiple SLF4J bindings.\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
                        "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
                        "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
                        "log4j:WARN Please initialize the log4j system properly.\n",
                        "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "getInstance(**kwargs={}) called\n",
                        "kwargs is {'region_name': 'us-east-1'}\n",
                        "endpoint is s3(https://s3.amazonaws.com)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "import stage_claim_into_raw\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
                "df, path = stage_claim_into_raw.run(spark, 'test-lf-wm')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 132,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'s3://test-lf-wm/etl/raw/claim_db/claim/full/202306041621/'"
                        ]
                    },
                    "execution_count": 132,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pathO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_result(path):\n",
                "    \"\"\"\n",
                "    Parse S3 path into bucket and key.\n",
                "    \"\"\"\n",
                "    path = path.lower().strip()\n",
                "    components = path.split('/')\n",
                "    return {\n",
                "        'path' : path,\n",
                "        'bucket' : components[2],\n",
                "        'tier' : components[4],\n",
                "        'table' : components[6],\n",
                "        'key' : '/'.join(components[4:][:-1]),\n",
                "        'env': components[2].split('-')[0]\n",
                "    }\n",
                "    \n",
                "pathO = path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# GLUE AUTOMATION UTILITIES\n",
                "# ----------------------------------------------------------------------------\n",
                "def get_job_name(path):\n",
                "    \"\"\"\n",
                "    Get the job name for the DataBrew profile job.\n",
                "    \"\"\"\n",
                "    return f\"{path['env']}-{path['tier']}-{path['table']}\"\n",
                "\n",
                "def get_table_name(path):\n",
                "    \"\"\"\n",
                "    Get the table name for the DataBrew profile job.\n",
                "    \"\"\"\n",
                "    return f\"{path['tier']}{path['table']}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('test-raw-claim', 'rawclaim')"
                        ]
                    },
                    "execution_count": 56,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_job_name(path), get_table_name(path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 115,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Job 'profile-test-raw-claim' already exists\n"
                    ]
                }
            ],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# CREATE PROFILE JOB\n",
                "# ----------------------------------------------------------------------------\n",
                "try: \n",
                "    databrew.create_profile_job(get_job_name(path),\n",
                "                            f\"profile-{get_job_name(path)}\", \n",
                "                            bucket='test-lf-wm', \n",
                "                            key='dataquality')\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' already exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 116,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset 'test-raw' already exists.\n"
                    ]
                }
            ],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# CREATE DATASET FOR DATA CATALOG ITEM\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    job = get_job_name(path)\n",
                "    table = get_table_name(path)\n",
                "    databrew.create_dataset(name=job, \n",
                "                        table=table, \n",
                "                        database='test-release-3', \n",
                "                        CatalogId='618572314333')\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Dataset '{name}' already exists.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 121,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Job 'profile-test-raw-claim' is already running\n"
                    ]
                }
            ],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# RUN PROFILE JOB\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    databrew.start_profile_job(f\"profile-{get_job_name(path)}\")\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' is already running\")\n",
                "except databrew.client.exceptions.ResourceNotFoundException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' does not exist\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 124,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_profile_object_path(job_name):\n",
                "    \"\"\"\n",
                "    Get the profile object for a given job name.\n",
                "    \"\"\"\n",
                "    latest_run = databrew.client.list_job_runs(Name=job_name)['JobRuns'][0]\n",
                "    # Get location information.\n",
                "    location = latest_run['Outputs'][0]['Location']\n",
                "    # Get path\n",
                "    bucket, key = location['Bucket'], location['Key']\n",
                "    return bucket,key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_profile_link(dataset):\n",
                "    return f\"https://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset={dataset}&tab=profile-overview\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 127,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('test-lf-wm',\n",
                            " 'dataquality/test-raw-claim_672988221f4998bda2c49474bb323ba1898599bb7895830747936916f9269bb3.json')"
                        ]
                    },
                    "execution_count": 127,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_profile_object_path(job_name=f\"profile-test-raw-claim\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "    \"Attempt\": 0,\n",
                        "    \"CompletedOn\": \"2023-06-04 08:00:20.610000+00:00\",\n",
                        "    \"DatasetName\": \"test-raw-claim\",\n",
                        "    \"ExecutionTime\": 135,\n",
                        "    \"JobName\": \"profile-test-raw-claim\",\n",
                        "    \"RunId\": \"db_e8f23d26addbdb1da6258f6a086e99bd3724deded967ad1deef4682849b04386\",\n",
                        "    \"State\": \"SUCCEEDED\",\n",
                        "    \"LogSubscription\": \"DISABLE\",\n",
                        "    \"Outputs\": [\n",
                        "        {\n",
                        "            \"Format\": \"JSON\",\n",
                        "            \"Location\": {\n",
                        "                \"Bucket\": \"test-lf-wm\",\n",
                        "                \"Key\": \"dataquality/test-raw-claim_e8f23d26addbdb1da6258f6a086e99bd3724deded967ad1deef4682849b04386.json\",\n",
                        "                \"BucketOwner\": \"618572314333\"\n",
                        "            },\n",
                        "            \"Overwrite\": false\n",
                        "        }\n",
                        "    ],\n",
                        "    \"StartedBy\": \"arn:aws:iam::618572314333:user/Lead\",\n",
                        "    \"StartedOn\": \"2023-06-04 07:57:54.526000+00:00\",\n",
                        "    \"JobSample\": {\n",
                        "        \"Mode\": \"CUSTOM_ROWS\",\n",
                        "        \"Size\": 20000\n",
                        "    }\n",
                        "}\n",
                        "SUCCEEDED\n"
                    ]
                }
            ],
            "source": [
                "for job in databrew.client.list_job_runs(Name=f\"profile-{get_job_name(path)}\")['JobRuns']:\n",
                "    finished_running = True\n",
                "    if job['State'] != 'SUCCEEDED':\n",
                "        finished_running = False\n",
                "    print(json.dumps(job, indent=4,default=str))\n",
                "    print(job['State'])\n",
                "    print(get_result(job['Outputs']['Location']['Key']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "\n",
                "# Create a CloudWatch Logs client\n",
                "cloudwatch_logs = boto3.client('logs', region_name='us-east-1')\n",
                "\n",
                "# Get the log events\n",
                "response = cloudwatch_logs.get_log_events(\n",
                "    logGroupName='/aws-glue/crawlers',\n",
                "    logStreamName='test-raw',\n",
                ")\n",
                "\n",
                "# Print the log events\n",
                "for event in response['events']:\n",
                "    print(event['timestamp'], event['message'])\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Link to profile report as part of logging.\n",
                "https://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=landing-claimdb-policyholder&tab=profile-overview\n",
                "\n",
                "`https://us-east-1.console.aws.amazon.com/databrew/home?region={region}#dataset-details?dataset={profile-job-name}&tab=profile-overview`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

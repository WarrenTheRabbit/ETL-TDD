{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Reset every staging tier except for Landing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "delete: s3://project-lf/etl/raw/claim_db/claim/full/202306022049/part-00000-27a8b021-6da7-4b8d-a14c-3c234baaed99-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/policyholder/full/202306022106/part-00000-df95f6cc-aee4-4573-bcad-39dd5ffd1428-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/provider/full/202306020108/part-00000-a369051d-e5fc-42d2-93b6-cec2238eb804-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/claim/full/202306020108/part-00000-b6fbd60f-8a52-4f92-b8c7-8699e5c91912-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/provider/full/202306021618/part-00000-a8b75da5-7fdd-4cbb-a31a-1a92ffc628c5-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/provider/full/202306022105/part-00000-a9b88f44-2248-4d91-8327-c5122571239e-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/policyholder/full/202306020108/part-00000-3cfb6d4a-0dda-4701-8577-dbb02b998dc4-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/provider/full/202306022055/part-00000-b90ae805-2240-4b14-a107-742cb505a552-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/claim/full/202306022107/part-00000-d49ce2a4-7592-495f-b7ea-3c00e056ba7a-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/raw/claim_db/provider/full/202306021630/part-00000-bee187bd-e1b9-4c4f-af72-f4e1af9ee90d-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/policyholder/full/202306020110/part-00000-eb2f29c7-f5ae-4d74-97e1-98dac0cf65ae-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/claim/full/202306020110/part-00000-390a99a4-342e-4ba5-b7d9-af2f758d2fc7-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/provider/full/202306020110/part-00000-82179b9a-569b-4671-8f23-30a4d269f549-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/policyholder/full/202306022106/part-00000-0a6e10f0-0e46-4b5f-92f1-551ee4734456-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/provider/full/202306021630/part-00000-c3ce1b89-bcc4-47f1-a0fd-36911da9c653-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/claim/full/202306022050/part-00000-2963192e-e8c4-4933-bc58-0a1378213328-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/provider/full/202306021619/part-00000-a65fb5b3-4f8b-402e-bb9e-47bcb2537c45-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/provider/full/202306022106/part-00000-a7689da2-02dd-49db-ae94-68a0be6bdac7-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/claim/full/202306022107/part-00000-0673434e-42b1-4669-ad76-fd13b1e99ac9-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/access/claim_db/provider/full/202306022055/part-00000-a7c35dda-32fa-4897-9f7b-227ab18316b6-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306020109/part-00002-13033745-411a-42cc-8061-f661770e0081-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/claim_fact/full/202306020113/part-00000-579af0f5-9dda-455f-b638-030a3aa9004a-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306020109/part-00000-13033745-411a-42cc-8061-f661770e0081-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306020109/part-00003-13033745-411a-42cc-8061-f661770e0081-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306022106/part-00001-3927abc1-6108-4c93-b8e3-461133827e22-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306022106/part-00003-3927abc1-6108-4c93-b8e3-461133827e22-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306022106/part-00002-3927abc1-6108-4c93-b8e3-461133827e22-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/claim_fact/full/202306022110/part-00000-1929a86a-9190-4925-942a-d6fac13186fb-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306020109/part-00001-13033745-411a-42cc-8061-f661770e0081-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/location_dim/full/202306020111/part-00000-ca5a0c4f-9362-45bb-9764-fb1c706c5385-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/location_dim/full/202306022109/part-00000-63a2557b-a309-405d-abc6-56a3feb7f7a7-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/provider_dim/full/202306020112/part-00000-35aacd5a-c3fe-4751-a105-dec876a1de72-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/date_dim/full/202306022106/part-00000-3927abc1-6108-4c93-b8e3-461133827e22-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/policyholder_dim/full/202306022109/part-00000-6f1db0d6-5114-49d3-965c-a8ae110b2d03-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/procedure_dim/full/202306022108/part-00000-0f9229a1-b2cd-47a0-b5c8-17f08a558fb9-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/policyholder_dim/full/202306020112/part-00000-200f1426-da49-4759-9946-00e5520f5fd6-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/procedure_dim/full/202306020111/part-00000-a9ef6895-b4a0-41a2-91e4-2e1a33ffb4c8-c000.snappy.parquet\n",
                        "delete: s3://project-lf/etl/optimised/provider_dim/full/202306022110/part-00000-de597aa8-99fc-420b-8fad-82622bfd29d9-c000.snappy.parquet\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 rm s3://project-lf/etl/raw --recursive\n",
                "!aws s3 rm s3://project-lf/etl/access --recursive\n",
                "!aws s3 rm s3://project-lf/etl/optimised --recursive"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load source data into `Bucket.TEST` and `Bucket.PROD` if needed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2023-06-03 00:21:47      22655 etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "2023-06-03 00:21:51       5818 etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "2023-06-03 00:21:55        814 etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 ls s3://project-lf/etl/landing --recursive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "upload: ../etl/mock/source_files/202305211851-claim-full.csv to s3://project-lf/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "upload: ../etl/mock/source_files/202305221132-policyholder-full.csv to s3://project-lf/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "upload: ../etl/mock/source_files/202305221136-provider-full.csv to s3://project-lf/etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305211851-claim-full.csv s3://project-lf/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                "\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221132-policyholder-full.csv s3://project-lf/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                "\n",
                "!aws s3 cp /home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221136-provider-full.csv s3://project-lf/etl/landing/claim_db/provider/full/202305221136-provider-full.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "test-lf-wm etl/landing/claim_db/claim/full/202305211851-claim-full.csv\n",
                        "test-lf-wm etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\n",
                        "test-lf-wm etl/landing/claim_db/provider/full/202305221136-provider-full.csv\n"
                    ]
                }
            ],
            "source": [
                "env = Bucket.TEST\n",
                "\n",
                "files = {\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305211851-claim-full.csv\": f\"s3://{env}/etl/landing/claim_db/claim/full/202305211851-claim-full.csv\",\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221132-policyholder-full.csv\": f\"s3://{env}/etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv\",\n",
                "    \"/home/glue_user/project_lf/ETL-TDD/etl/mock/source_files/202305221136-provider-full.csv\": f\"s3://{env}/etl/landing/claim_db/provider/full/202305221136-provider-full.csv\",\n",
                "}\n",
                "\n",
                "# s3_resource = S3Resource.getInstance()\n",
                "\n",
                "for src, dest in files.items():\n",
                "\n",
                "    print(bucket_name, key)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Delete every glue job"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [],
            "source": [
                "# delete all glue ETL jobs\n",
                "glue = boto3.client('glue', region_name='us-east-1')\n",
                "jobs = glue.get_jobs()\n",
                "for job in jobs['Jobs']:\n",
                "    if job['Name'].startswith('stage_'):\n",
                "        glue.delete_job(JobName=job['Name'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refresh every .py job file in `code/jobs/` and `code/dependencies` "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# delete the libraries.zip file at s3://project-lf/code/libraries.zip\n",
                "!aws s3 rm s3://project-lf/code/libraries.zip\n",
                "\n",
                "# create a new zip file\n",
                "!zip -r libraries.zip *\n",
                "\n",
                "# write cli that copies libraries.zip to s3://project-lf/code/libraries.zip\n",
                "!aws s3 cp libraries.zip s3://project-lf/code/dependencies/libraries.zip\n",
                "\n",
                "# delete everything but the libraries.zip file at s3://project-lf/code/\n",
                "!aws s3 rm s3://project-lf/code/ --recursive --exclude \"libraries.zip\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "upload: ./libraries.zip to s3://project-lf/code/dependencies/libraries.zip\n"
                    ]
                }
            ],
            "source": [
                "# write cli that copies libraries.zip to s3://project-lf/code/libraries.zip\n",
                "!aws s3 cp libraries.zip s3://project-lf/code/dependencies/libraries.zip\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload .py job files to S3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Uploaded 13 files to s3://project-lf/code/jobs/\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "# Create an S3 client\n",
                "s3_client = boto3.client('s3')\n",
                "\n",
                "# Set the bucket and prefix\n",
                "bucket = 'project-lf'\n",
                "prefix = 'code/jobs/'\n",
                "\n",
                "# Set the list of files to upload\n",
                "files = [\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_claim_into_access.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_claim_into_optimised.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_claim_into_raw.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_date_into_optimised.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_location_into_optimised.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_policyholder_into_access.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_policyholder_into_optimised.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_policyholder_into_raw.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_procedure_into_optimised.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_provider_into_access.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_provider_into_optimised.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_provider_into_raw.py',\n",
                "    '/home/glue_user/project_lf/ETL-TDD/stage_source_into_landing.py'\n",
                "]\n",
                "\n",
                "# Upload each file to the S3 bucket\n",
                "for file_path in files:\n",
                "    # Get the file name\n",
                "    file_name = file_path.split('/')[-1]\n",
                "\n",
                "    # Set the object key\n",
                "    key = f'{prefix}{file_name}'\n",
                "\n",
                "    # Upload the file\n",
                "    s3_client.upload_file(file_path, bucket, key)\n",
                "\n",
                "print(f'Uploaded {len(files)} files to s3://{bucket}/{prefix}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialise Glue session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for each script in s3://project-lf/code/jobs/ create a glue job\n",
                "import boto3\n",
                "\n",
                "# Create an AWS Glue client\n",
                "glue_client = boto3.client('glue', region_name='us-east-1')\n",
                "\n",
                "# Set the parameters for the new Glue job\n",
                "glue_role = 'data-quality-lf'\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get a list of all script objects in `project-lf/code/jobs/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_claim_into_access.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_claim_into_optimised.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_claim_into_raw.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_date_into_optimised.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_location_into_optimised.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_policyholder_into_access.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_policyholder_into_optimised.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_policyholder_into_raw.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_procedure_into_optimised.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_provider_into_access.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_provider_into_optimised.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_provider_into_raw.py'),\n",
                            " s3.ObjectSummary(bucket_name='project-lf', key='code/jobs/stage_source_into_landing.py')]"
                        ]
                    },
                    "execution_count": 86,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# get all keys in s3://project-lf/code/jobs/\n",
                "s3_resource = boto3.resource('s3')\n",
                "bucket = 'project-lf'\n",
                "prefix = 'code/jobs/'\n",
                "objects = s3_resource.Bucket(bucket).objects.filter(Prefix=prefix)\n",
                "list(objects)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create a job for every script object"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "code/jobs/stage_claim_into_access.py\n",
                        "{'Name': 'stage_claim_into_access', 'ResponseMetadata': {'RequestId': 'a8d51c8f-04f6-4376-952d-615c1a7af271', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:34 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '34', 'connection': 'keep-alive', 'x-amzn-requestid': 'a8d51c8f-04f6-4376-952d-615c1a7af271'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_claim_into_optimised.py\n",
                        "{'Name': 'stage_claim_into_optimised', 'ResponseMetadata': {'RequestId': 'b3365302-8774-4f81-a2b7-86a8f77b9afe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:35 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '37', 'connection': 'keep-alive', 'x-amzn-requestid': 'b3365302-8774-4f81-a2b7-86a8f77b9afe'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_claim_into_raw.py\n",
                        "{'Name': 'stage_claim_into_raw', 'ResponseMetadata': {'RequestId': '4705c06e-779c-4e6f-bad6-2573f07ce73d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:35 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '31', 'connection': 'keep-alive', 'x-amzn-requestid': '4705c06e-779c-4e6f-bad6-2573f07ce73d'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_date_into_optimised.py\n",
                        "{'Name': 'stage_date_into_optimised', 'ResponseMetadata': {'RequestId': 'b8bd45a3-7383-4768-bb37-8bfdb6f61772', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '36', 'connection': 'keep-alive', 'x-amzn-requestid': 'b8bd45a3-7383-4768-bb37-8bfdb6f61772'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_location_into_optimised.py\n",
                        "{'Name': 'stage_location_into_optimised', 'ResponseMetadata': {'RequestId': 'e38b7d2f-1309-4554-a461-6ebb969a8f00', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '40', 'connection': 'keep-alive', 'x-amzn-requestid': 'e38b7d2f-1309-4554-a461-6ebb969a8f00'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_policyholder_into_access.py\n",
                        "{'Name': 'stage_policyholder_into_access', 'ResponseMetadata': {'RequestId': '60de252c-2cd5-45b9-b730-132930893e1b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '41', 'connection': 'keep-alive', 'x-amzn-requestid': '60de252c-2cd5-45b9-b730-132930893e1b'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_policyholder_into_optimised.py\n",
                        "{'Name': 'stage_policyholder_into_optimised', 'ResponseMetadata': {'RequestId': 'e25e1785-b585-47f4-8683-1c072f0a17fa', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:37 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '44', 'connection': 'keep-alive', 'x-amzn-requestid': 'e25e1785-b585-47f4-8683-1c072f0a17fa'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_policyholder_into_raw.py\n",
                        "{'Name': 'stage_policyholder_into_raw', 'ResponseMetadata': {'RequestId': '87a19388-a3b5-4225-b4c8-4c862ee6b4a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:37 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '38', 'connection': 'keep-alive', 'x-amzn-requestid': '87a19388-a3b5-4225-b4c8-4c862ee6b4a4'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_procedure_into_optimised.py\n",
                        "{'Name': 'stage_procedure_into_optimised', 'ResponseMetadata': {'RequestId': '4ca63849-2687-4bd6-864a-42664682c087', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '41', 'connection': 'keep-alive', 'x-amzn-requestid': '4ca63849-2687-4bd6-864a-42664682c087'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_provider_into_access.py\n",
                        "{'Name': 'stage_provider_into_access', 'ResponseMetadata': {'RequestId': '0bc1635d-7366-47ce-a31c-495139496dbb', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '37', 'connection': 'keep-alive', 'x-amzn-requestid': '0bc1635d-7366-47ce-a31c-495139496dbb'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_provider_into_optimised.py\n",
                        "{'Name': 'stage_provider_into_optimised', 'ResponseMetadata': {'RequestId': '9e855a74-3124-4305-be30-7126ece8a069', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '40', 'connection': 'keep-alive', 'x-amzn-requestid': '9e855a74-3124-4305-be30-7126ece8a069'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_provider_into_raw.py\n",
                        "{'Name': 'stage_provider_into_raw', 'ResponseMetadata': {'RequestId': '3840f01f-18a4-4ecb-b203-1b4817f02808', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:39 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '34', 'connection': 'keep-alive', 'x-amzn-requestid': '3840f01f-18a4-4ecb-b203-1b4817f02808'}, 'RetryAttempts': 0}}\n",
                        "code/jobs/stage_source_into_landing.py\n",
                        "{'Name': 'stage_source_into_landing', 'ResponseMetadata': {'RequestId': '7232af30-eda7-4b76-ae1e-a1a68445fd85', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 01 Jun 2023 14:55:41 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '36', 'connection': 'keep-alive', 'x-amzn-requestid': '7232af30-eda7-4b76-ae1e-a1a68445fd85'}, 'RetryAttempts': 0}}\n"
                    ]
                }
            ],
            "source": [
                "glue_client = boto3.client('glue', region_name='us-east-1')\n",
                "job_names = []\n",
                "for s3_object in objects:\n",
                "    print(s3_object.key)\n",
                "    \n",
                "    # Create an AWS Glue client\n",
                "\n",
                "    # Set the parameters for the new Glue job\n",
                "    glue_role = 'data-quality-lf'\n",
                "    script_location = f's3://project-lf/{s3_object.key}'\n",
                "    job_name = script_location.split('/')[-1].split('.')[0]\n",
                "    extra_py_files = 's3://project-lf/code/dependencies/libraries.zip'\n",
                "    glue_version = '4.0'\n",
                "    additional_python_modules = \"holidays\"\n",
                "\n",
                "    try:\n",
                "        # Create the new Glue job\n",
                "        response = glue_client.create_job(\n",
                "            Name=job_name,\n",
                "            Role=glue_role,\n",
                "            Command={'Name': 'glueetl', 'ScriptLocation': script_location},\n",
                "            GlueVersion=glue_version,\n",
                "            DefaultArguments={\n",
                "                '--extra-py-files': extra_py_files,\n",
                "                '--additional-python-modules': additional_python_modules\n",
                "            },\n",
                "            WorkerType='G.1X',\n",
                "            NumberOfWorkers=2\n",
                "        )\n",
                "        print(response)\n",
                "    except Exception as e:\n",
                "        print(\"Skipping job creation for\", job_name, e)\n",
                "    \n",
                "    job_names.append(job_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define execution order of jobs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [],
            "source": [
                "job_waves = [\n",
                "     ['stage_claim_into_raw', 'stage_provider_into_raw', 'stage_policyholder_into_raw',\n",
                "    'stage_date_into_optimised'],\n",
                "     \n",
                "    ['stage_claim_into_access','stage_provider_into_access',\n",
                "        'stage_policyholder_into_access'],\n",
                "    \n",
                "    ['stage_location_into_optimised', 'stage_procedure_into_optimised'],\n",
                "    \n",
                "    ['stage_policyholder_into_optimised', 'stage_provider_into_optimised'],\n",
                "    \n",
                "    ['stage_claim_into_optimised']\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define a function to run (in parallel) all jobs in a list and wait until they have completed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "import time\n",
                "import colorama\n",
                "from colorama import Fore, Style\n",
                "colorama.init()\n",
                "\n",
                "# Create an AWS Glue client\n",
                "glue_client = boto3.client('glue', region_name='us-east-1')\n",
                "\n",
                "# Function to run a list of jobs and wait for them to finish\n",
                "def run_jobs(jobs):\n",
                "    # Start each job and store its job run ID\n",
                "    job_run_ids = {}\n",
                "    for job_name in jobs:\n",
                "        response = glue_client.start_job_run(JobName=job_name)\n",
                "        job_run_id = response['JobRunId']\n",
                "        job_run_ids[job_name] = job_run_id\n",
                "        print(job_name, job_run_id)\n",
                "\n",
                "    print(\"-\"*80)\n",
                "    \n",
                "    # Wait for all jobs to finish\n",
                "    while True:\n",
                "        # Check the status of each job run\n",
                "        all_finished = True\n",
                "    \n",
                "        for job_name, job_run_id in job_run_ids.items():\n",
                "            try:\n",
                "                response = glue_client.get_job(JobName=job_name)\n",
                "                response = glue_client.get_job_run(JobName=job_name, RunId=job_run_id)\n",
                "                status = response['JobRun']['JobRunState']\n",
                "                if status not in ['SUCCEEDED', 'FAILED', 'STOPPED']:\n",
                "                    all_finished = False\n",
                "                else:\n",
                "                    if status == 'SUCCEEDED':\n",
                "                        print(Fore.GREEN + '\\t' + job_name + ' has ' + status + '.' + Style.RESET_ALL)\n",
                "                    elif status == 'FAILED':\n",
                "                        error_message = response['JobRun'].get('ErrorMessage', 'No error message available')\n",
                "                        print(Fore.RED + '\\t' + job_name + ' has ' + status + '.' + Style.RESET_ALL)\n",
                "                        print(Fore.RED + f'\\t{job_name} has {status}. Error message: {error_message}' + Style.RESET_ALL)\n",
                "                    else:\n",
                "                        print(Fore.YELLOW + f'\\t{job_name} has {status}.' + Style.RESET_ALL)\n",
                "            except glue_client.exceptions.EntityNotFoundException:\n",
                "                continue\n",
                "        print(\"-\"*80)\n",
                "        # If all jobs have finished, exit the loop\n",
                "        if all_finished:\n",
                "            break\n",
                "        # Otherwise, wait and check again\n",
                "        time.sleep(10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execute jobs in order"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "********************************************************************************\n",
                        "Starting on Run 1: ['stage_claim_into_raw', 'stage_provider_into_raw', 'stage_policyholder_into_raw', 'stage_date_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_claim_into_raw jr_f013cdcd5ba3156e87729f26f19dc25d5ebcffcf263ab4858c02f2b56e4d5776\n",
                        "stage_provider_into_raw jr_06ab19d466f1cb9a05dd8fe770393be13bebf127b9434129619e81282a2d5166\n",
                        "stage_policyholder_into_raw jr_fdc34733a113a4eaba4bb9a66582be5bf7208a1807792cd3f7bcc72d1b389858\n",
                        "stage_date_into_optimised jr_0a425aa1c16043e5e8f600a973c86f7a4d4890574cd22b8b1a073d1428696990\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_provider_into_rawhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_claim_into_rawhas SUCCEEDED.\n",
                        "\tstage_provider_into_rawhas SUCCEEDED.\n",
                        "\tstage_policyholder_into_rawhas SUCCEEDED.\n",
                        "\tstage_date_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 2: ['stage_claim_into_access', 'stage_provider_into_access', 'stage_policyholder_into_access']\n",
                        "********************************************************************************\n",
                        "stage_claim_into_access jr_d13a12e9e73dcd8f3d165860b44f0ef7b384e14fe0c28f542098e5dae4f8a6ca\n",
                        "stage_provider_into_access jr_f9525163d5adcce6169f188a173c3431125844e6f8a646671fa85d34eefa0a1b\n",
                        "stage_policyholder_into_access jr_74ca915757abcd9ea065bccd4e702a3dfbfc05d933440aac54326c85f8201927\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_provider_into_accesshas SUCCEEDED.\n",
                        "\tstage_policyholder_into_accesshas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_claim_into_accesshas SUCCEEDED.\n",
                        "\tstage_provider_into_accesshas SUCCEEDED.\n",
                        "\tstage_policyholder_into_accesshas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 3: ['stage_location_into_optimised', 'stage_procedure_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_location_into_optimised jr_a5a6881e36bd0f6041043ceff6336e2ad599bcf05abb80ce895e11ea5fb083a9\n",
                        "stage_procedure_into_optimised jr_950089f62361213c7733e66546bd76bdc03e7fbc7ad01a0033873eea62d9c5b1\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_location_into_optimisedhas SUCCEEDED.\n",
                        "\tstage_procedure_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 4: ['stage_policyholder_into_optimised', 'stage_provider_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_policyholder_into_optimised jr_9ada937f0b9b610b1a15c408abee6ac7bea1ea32ced64b5fce767645e4d2c654\n",
                        "stage_provider_into_optimised jr_a1c965cce6124508bcfd00b5d6578a3e953d20adc65894a005afd779da1aab3b\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_policyholder_into_optimisedhas SUCCEEDED.\n",
                        "\tstage_provider_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "********************************************************************************\n",
                        "Starting on Run 5: ['stage_claim_into_optimised']\n",
                        "********************************************************************************\n",
                        "stage_claim_into_optimised jr_52541dcad8424000d6d67dc8dccdaebfa593089c70404723707489d773f280f8\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "--------------------------------------------------------------------------------\n",
                        "\tstage_claim_into_optimisedhas SUCCEEDED.\n",
                        "--------------------------------------------------------------------------------\n",
                        "All jobs finished\n"
                    ]
                }
            ],
            "source": [
                "for count, job_set in enumerate(job_waves):\n",
                "    print(\"*\"*80)\n",
                "    print(f\"Starting on Run {count + 1}: {job_set}\")\n",
                "    print(\"*\"*80)\n",
                "    run_jobs(job_set)\n",
                "print('All jobs finished')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment with boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ClientError",
                    "evalue": "An error occurred (AccessDenied) when calling the CopyObject operation: Access Denied",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[11], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m dataset_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39myour-dataset-name\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Stage the file\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m stage_file(bucket_name, file_key, staging_bucket)\n\u001b[1;32m     59\u001b[0m \u001b[39m# Run the Glue crawler\u001b[39;00m\n\u001b[1;32m     60\u001b[0m run_crawler(crawler_name)\n",
                        "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mstage_file\u001b[0;34m(bucket_name, file_key, staging_bucket)\u001b[0m\n\u001b[1;32m      4\u001b[0m s3_client \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39ms3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Copy the file from the source bucket to the staging bucket\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m s3_client\u001b[39m.\u001b[39;49mcopy_object(\n\u001b[1;32m      8\u001b[0m     Bucket\u001b[39m=\u001b[39;49mstaging_bucket,\n\u001b[1;32m      9\u001b[0m     CopySource\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mBucket\u001b[39;49m\u001b[39m'\u001b[39;49m: bucket_name, \u001b[39m'\u001b[39;49m\u001b[39mKey\u001b[39;49m\u001b[39m'\u001b[39;49m: file_key},\n\u001b[1;32m     10\u001b[0m     Key\u001b[39m=\u001b[39;49mfile_key\n\u001b[1;32m     11\u001b[0m )\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:514\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:938\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
                        "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the CopyObject operation: Access Denied"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "def stage_file(bucket_name, file_key, staging_bucket):\n",
                "    s3_client = boto3.client('s3')\n",
                "\n",
                "    # Copy the file from the source bucket to the staging bucket\n",
                "    s3_client.copy_object(\n",
                "        Bucket=staging_bucket,\n",
                "        CopySource={'Bucket': bucket_name, 'Key': file_key},\n",
                "        Key=file_key\n",
                "    )\n",
                "\n",
                "def run_crawler(crawler_name):\n",
                "    glue_client = boto3.client('glue')\n",
                "\n",
                "    # Start the Glue crawler\n",
                "    response = glue_client.start_crawler(Name=crawler_name)\n",
                "\n",
                "    # Wait for the crawler to complete\n",
                "    glue_client.get_waiter('crawler_complete').wait(\n",
                "        Name=crawler_name,\n",
                "        WaiterConfig={'Delay': 2, 'MaxAttempts': 100}\n",
                "    )\n",
                "\n",
                "def run_databrew_profile_job(job_name, dataset_name):\n",
                "    databrew_client = boto3.client('databrew')\n",
                "\n",
                "    # Create a new AWS Glue DataBrew profile job run\n",
                "    response = databrew_client.create_job_run(\n",
                "        Name=job_name,\n",
                "        Type='PROFILE',\n",
                "        DatasetName=dataset_name\n",
                "    )\n",
                "\n",
                "    # Get the job run ID\n",
                "    job_run_id = response['JobRunId']\n",
                "    print(f\"Started DataBrew profile job run. Job Run ID: {job_run_id}\")\n",
                "\n",
                "    # Wait for the job run to complete\n",
                "    databrew_client.get_waiter('job_run_state_change').wait(\n",
                "        Name=job_name,\n",
                "        RunId=job_run_id,\n",
                "        WaiterConfig={'Delay': 10, 'MaxAttempts': 60}\n",
                "    )\n",
                "\n",
                "    print(\"DataBrew profile job run completed.\")\n",
                "\n",
                "# Example usage\n",
                "bucket_name = 'your-source-bucket'\n",
                "file_key = 'path/to/file.csv'\n",
                "staging_bucket = 'your-staging-bucket'\n",
                "crawler_name = 'your-crawler-name'\n",
                "databrew_job_name = 'your-databrew-job-name'\n",
                "dataset_name = 'your-dataset-name'\n",
                "\n",
                "# Stage the file\n",
                "stage_file(bucket_name, file_key, staging_bucket)\n",
                "\n",
                "# Run the Glue crawler\n",
                "run_crawler(crawler_name)\n",
                "\n",
                "# Run the DataBrew profile job on the staged file\n",
                "run_databrew_profile_job(databrew_job_name, dataset_name)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
                "# SPDX-License-Identifier: Apache-2.0\n",
                "\n",
                "\"\"\"\n",
                "Purpose\n",
                "\n",
                "Shows how to use the AWS SDK for Python (Boto3) with AWS Glue to\n",
                "create and manage crawlers, databases, and jobs.\n",
                "\"\"\"\n",
                "\n",
                "import logging\n",
                "from botocore.exceptions import ClientError\n",
                "\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "# snippet-start:[python.example_code.glue.GlueWrapper.full]\n",
                "# snippet-start:[python.example_code.glue.GlueWrapper.decl]\n",
                "class GlueWrapper:\n",
                "    \"\"\"Encapsulates AWS Glue actions.\"\"\"\n",
                "    def __init__(self, glue_client):\n",
                "        \"\"\"\n",
                "        :param glue_client: A Boto3 Glue client.\n",
                "        \"\"\"\n",
                "        self.glue_client = glue_client\n",
                "# snippet-end:[python.example_code.glue.GlueWrapper.decl]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetCrawler]\n",
                "    def get_crawler(self, name):\n",
                "        \"\"\"\n",
                "        Gets information about a crawler.\n",
                "\n",
                "        :param name: The name of the crawler to look up.\n",
                "        :return: Data about the crawler.\n",
                "        \"\"\"\n",
                "        crawler = None\n",
                "        try:\n",
                "            response = self.glue_client.get_crawler(Name=name)\n",
                "            crawler = response['Crawler']\n",
                "        except ClientError as err:\n",
                "            if err.response['Error']['Code'] == 'EntityNotFoundException':\n",
                "                logger.info(\"Crawler %s doesn't exist.\", name)\n",
                "            else:\n",
                "                logger.error(\n",
                "                    \"Couldn't get crawler %s. Here's why: %s: %s\", name,\n",
                "                    err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "                raise\n",
                "        return crawler\n",
                "    # snippet-end:[python.example_code.glue.GetCrawler]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.CreateCrawler]\n",
                "    def create_crawler(self, name, role_arn, db_name, db_prefix, s3_target):\n",
                "        \"\"\"\n",
                "        Creates a crawler that can crawl the specified target and populate a\n",
                "        database in your AWS Glue Data Catalog with metadata that describes the data\n",
                "        in the target.\n",
                "\n",
                "        :param name: The name of the crawler.\n",
                "        :param role_arn: The Amazon Resource Name (ARN) of an AWS Identity and Access\n",
                "                         Management (IAM) role that grants permission to let AWS Glue\n",
                "                         access the resources it needs.\n",
                "        :param db_name: The name to give the database that is created by the crawler.\n",
                "        :param db_prefix: The prefix to give any database tables that are created by\n",
                "                          the crawler.\n",
                "        :param s3_target: The URL to an S3 bucket that contains data that is\n",
                "                          the target of the crawler.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.create_crawler(\n",
                "                Name=name,\n",
                "                Role=role_arn,\n",
                "                DatabaseName=db_name,\n",
                "                TablePrefix=db_prefix,\n",
                "                Targets={'S3Targets': [{'Path': s3_target}]})\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't create crawler. Here's why: %s: %s\",\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.CreateCrawler]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.StartCrawler]\n",
                "    def start_crawler(self, name):\n",
                "        \"\"\"\n",
                "        Starts a crawler. The crawler crawls its configured target and creates\n",
                "        metadata that describes the data it finds in the target data source.\n",
                "\n",
                "        :param name: The name of the crawler to start.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.start_crawler(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't start crawler %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.StartCrawler]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetDatabase]\n",
                "    def get_database(self, name):\n",
                "        \"\"\"\n",
                "        Gets information about a database in your Data Catalog.\n",
                "\n",
                "        :param name: The name of the database to look up.\n",
                "        :return: Information about the database.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_database(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get database %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['Database']\n",
                "    # snippet-end:[python.example_code.glue.GetDatabase]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetTables]\n",
                "    def get_tables(self, db_name):\n",
                "        \"\"\"\n",
                "        Gets a list of tables in a Data Catalog database.\n",
                "\n",
                "        :param db_name: The name of the database to query.\n",
                "        :return: The list of tables in the database.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_tables(DatabaseName=db_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get tables %s. Here's why: %s: %s\", db_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['TableList']\n",
                "    # snippet-end:[python.example_code.glue.GetTables]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.CreateJob]\n",
                "    def create_job(self, name, description, role_arn, script_location):\n",
                "        \"\"\"\n",
                "        Creates a job definition for an extract, transform, and load (ETL) job that can\n",
                "        be run by AWS Glue.\n",
                "\n",
                "        :param name: The name of the job definition.\n",
                "        :param description: The description of the job definition.\n",
                "        :param role_arn: The ARN of an IAM role that grants AWS Glue the permissions\n",
                "                         it requires to run the job.\n",
                "        :param script_location: The Amazon S3 URL of a Python ETL script that is run as\n",
                "                                part of the job. The script defines how the data is\n",
                "                                transformed.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.create_job(\n",
                "                Name=name, Description=description, Role=role_arn,\n",
                "                Command={'Name': 'glueetl', 'ScriptLocation': script_location, 'PythonVersion': '3'},\n",
                "                GlueVersion='3.0')\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't create job %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.CreateJob]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.StartJobRun]\n",
                "    def start_job_run(self, name, input_database, input_table, output_bucket_name):\n",
                "        \"\"\"\n",
                "        Starts a job run. A job run extracts data from the source, transforms it,\n",
                "        and loads it to the output bucket.\n",
                "\n",
                "        :param name: The name of the job definition.\n",
                "        :param input_database: The name of the metadata database that contains tables\n",
                "                               that describe the source data. This is typically created\n",
                "                               by a crawler.\n",
                "        :param input_table: The name of the table in the metadata database that\n",
                "                            describes the source data.\n",
                "        :param output_bucket_name: The S3 bucket where the output is written.\n",
                "        :return: The ID of the job run.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            # The custom Arguments that are passed to this function are used by the\n",
                "            # Python ETL script to determine the location of input and output data.\n",
                "            response = self.glue_client.start_job_run(\n",
                "                JobName=name,\n",
                "                Arguments={\n",
                "                    '--input_database': input_database,\n",
                "                    '--input_table': input_table,\n",
                "                    '--output_bucket_url': f's3://{output_bucket_name}/'})\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't start job run %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobRunId']\n",
                "    # snippet-end:[python.example_code.glue.StartJobRun]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.ListJobs]\n",
                "    def list_jobs(self):\n",
                "        \"\"\"\n",
                "        Lists the names of job definitions in your account.\n",
                "\n",
                "        :return: The list of job definition names.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.list_jobs()\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't list jobs. Here's why: %s: %s\",\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobNames']\n",
                "    # snippet-end:[python.example_code.glue.ListJobs]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetJobRuns]\n",
                "    def get_job_runs(self, job_name):\n",
                "        \"\"\"\n",
                "        Gets information about runs that have been performed for a specific job\n",
                "        definition.\n",
                "\n",
                "        :param job_name: The name of the job definition to look up.\n",
                "        :return: The list of job runs.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_job_runs(JobName=job_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get job runs for %s. Here's why: %s: %s\", job_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobRuns']\n",
                "    # snippet-end:[python.example_code.glue.GetJobRuns]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.GetJobRun]\n",
                "    def get_job_run(self, name, run_id):\n",
                "        \"\"\"\n",
                "        Gets information about a single job run.\n",
                "\n",
                "        :param name: The name of the job definition for the run.\n",
                "        :param run_id: The ID of the run.\n",
                "        :return: Information about the run.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.glue_client.get_job_run(JobName=name, RunId=run_id)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't get job run %s/%s. Here's why: %s: %s\", name, run_id,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "        else:\n",
                "            return response['JobRun']\n",
                "    # snippet-end:[python.example_code.glue.GetJobRun]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteJob]\n",
                "    def delete_job(self, job_name):\n",
                "        \"\"\"\n",
                "        Deletes a job definition. This also deletes data about all runs that are\n",
                "        associated with this job definition.\n",
                "\n",
                "        :param job_name: The name of the job definition to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_job(JobName=job_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete job %s. Here's why: %s: %s\", job_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteJob]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteTable]\n",
                "    def delete_table(self, db_name, table_name):\n",
                "        \"\"\"\n",
                "        Deletes a table from a metadata database.\n",
                "\n",
                "        :param db_name: The name of the database that contains the table.\n",
                "        :param table_name: The name of the table to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_table(DatabaseName=db_name, Name=table_name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete table %s. Here's why: %s: %s\", table_name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteTable]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteDatabase]\n",
                "    def delete_database(self, name):\n",
                "        \"\"\"\n",
                "        Deletes a metadata database from your Data Catalog.\n",
                "\n",
                "        :param name: The name of the database to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_database(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete database %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteDatabase]\n",
                "\n",
                "    # snippet-start:[python.example_code.glue.DeleteCrawler]\n",
                "    def delete_crawler(self, name):\n",
                "        \"\"\"\n",
                "        Deletes a crawler.\n",
                "\n",
                "        :param name: The name of the crawler to delete.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            self.glue_client.delete_crawler(Name=name)\n",
                "        except ClientError as err:\n",
                "            logger.error(\n",
                "                \"Couldn't delete crawler %s. Here's why: %s: %s\", name,\n",
                "                err.response['Error']['Code'], err.response['Error']['Message'])\n",
                "            raise\n",
                "    # snippet-end:[python.example_code.glue.DeleteCrawler]\n",
                "# snippet-end:[python.example_code.glue.GlueWrapper.full]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Crawler\n",
                "https://docs.aws.amazon.com/glue/latest/dg/example_glue_CreateCrawler_section.html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Couldn't create crawler. Here's why: AlreadyExistsException: 618572314333:boto3-dq-crawler already exists\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "An error occurred (AlreadyExistsException) when calling the CreateCrawler operation: 618572314333:boto3-dq-crawler already exists\n",
                        "Crawler exists.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Couldn't start crawler boto3-dq-crawler. Here's why: CrawlerRunningException: Crawler with name boto3-dq-crawler has already started\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "An error occurred (CrawlerRunningException) when calling the StartCrawler operation: Crawler with name boto3-dq-crawler has already started\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"boto3-dq-crawler\"\n",
                "database_name = \"data-quality\"\n",
                "s3_target_path = \"s3://project-lf/data_quality\"\n",
                "\n",
                "glue = GlueWrapper(boto3.client('glue', region_name='us-east-1'))\n",
                "\n",
                "try:\n",
                "    response = glue.create_crawler(name, role, database_name, '', s3_target_path)\n",
                "    print(response)\n",
                "except glue.glue_client.exceptions.AlreadyExistsException as e:\n",
                "    print(e)\n",
                "    \n",
                "if glue.get_crawler(name) == None:\n",
                "    print(\"Crawler does not exist.\")\n",
                "else:\n",
                "    print(\"Crawler exists.\")\n",
                "    \n",
                "try:\n",
                "    response = glue.start_crawler(name)\n",
                "    print(response)\n",
                "except glue.glue_client.exceptions.CrawlerRunningException as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'Name': 'boto3-dq-crawler', 'Role': 'service-role/AWSGlueServiceRole-ETL', 'Targets': {'S3Targets': [{'Path': 's3://project-lf/data_quality', 'Exclusions': []}], 'JdbcTargets': [], 'MongoDBTargets': [], 'DynamoDBTargets': [], 'CatalogTargets': [], 'DeltaTargets': []}, 'DatabaseName': 'data-quality', 'Classifiers': [], 'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'}, 'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE', 'DeleteBehavior': 'DEPRECATE_IN_DATABASE'}, 'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'}, 'State': 'STOPPING', 'TablePrefix': 'aws_glue_', 'CrawlElapsedTime': 46000, 'CreationTime': datetime.datetime(2023, 6, 4, 0, 51, 11, tzinfo=tzlocal()), 'LastUpdated': datetime.datetime(2023, 6, 4, 0, 51, 11, tzinfo=tzlocal()), 'Version': 1, 'LakeFormationConfiguration': {'UseLakeFormationCredentials': False, 'AccountId': ''}}\n"
                    ]
                }
            ],
            "source": [
                "response = glue.get_crawler(name)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'ResponseMetadata': {'RequestId': '06c3ae2f-bb8b-4c49-bb95-b22564c3fe3d',\n",
                            "  'HTTPStatusCode': 200,\n",
                            "  'HTTPHeaders': {'date': 'Sun, 04 Jun 2023 00:51:47 GMT',\n",
                            "   'content-type': 'application/x-amz-json-1.1',\n",
                            "   'content-length': '2',\n",
                            "   'connection': 'keep-alive',\n",
                            "   'x-amzn-requestid': '06c3ae2f-bb8b-4c49-bb95-b22564c3fe3d'},\n",
                            "  'RetryAttempts': 0}}"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "create_crawler(name, role, database_name, s3_target_path)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create and execute landing tier crawler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"boto3-landing-production\"\n",
                "database_name = \"release-3\"\n",
                "prefix = \"landing\"\n",
                "s3_target_path = \"s3://project-lf/etl/landing\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = GlueWrapper(boto3.client('glue', region_name='us-east-1'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.create_crawler(name, role, database_name, '', s3_target_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.start_crawler(name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": [
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"test-boto3-landing-production\"\n",
                "database_name = \"release-3\"\n",
                "prefix = \"landing\"\n",
                "s3_target_path = \"s3://test-lf-wm/etl/landing\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create and run profile jobs of landing tier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "databrew = boto3.client('databrew', region_name='us-east-1')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ideas"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Wrapper for dq report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "def get_dq_results(bucket, key, s3_resource):\n",
                "    \"\"\"\n",
                "    Get the data quality results from the S3 bucket.\n",
                "    \"\"\"\n",
                "    s3_object = s3_resource.Object(bucket, key)\n",
                "    dq_results = s3_object.get()[\"Body\"].read().decode(\"utf-8\")\n",
                "    dict_results = json.loads(dq_results)\n",
                "\n",
                "    return dict_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bucket = 'project-lf'\n",
                "key = 'dataquality/landing-claimdb-policyholder_45af5c6917a253505482214c95eb38d832948c5aa6c3e80bf4efb71426938ea8.json'\n",
                "\n",
                "s3 = boto3.resource('s3', region_name='us-east-1')\n",
                "get_dq_results(bucket, key, s3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "get_dq_results(bucket, key, s3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ResourceNotFoundException",
                    "evalue": "An error occurred (ResourceNotFoundException) when calling the CreateProfileJob operation: Dataset landing-claimdb-provider wasn't found",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mResourceNotFoundException\u001b[0m                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[99], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m databrew \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39mdatabrew\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[39m=\u001b[39m databrew\u001b[39m.\u001b[39;49mcreate_profile_job(\n\u001b[1;32m      6\u001b[0m     DatasetName\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlanding-claimdb-provider\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     Name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mproduction-landing-claimdb-provider\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     OutputLocation\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      9\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mBucket\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mproject-lf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mKey\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mdataquality/\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m     },\n\u001b[1;32m     12\u001b[0m     RoleArn\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39marn:aws:iam::618572314333:role/DataBrew-Data-Quality-Workflows-DataAccessRole-ZRAZ5RGWCQW0\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:514\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:938\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
                        "\u001b[0;31mResourceNotFoundException\u001b[0m: An error occurred (ResourceNotFoundException) when calling the CreateProfileJob operation: Dataset landing-claimdb-provider wasn't found"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "\n",
                "databrew = boto3.client('databrew')\n",
                "\n",
                "response = databrew.create_profile_job(\n",
                "    DatasetName='landing-claimdb-provider',\n",
                "    Name='production-landing-claimdb-provider',\n",
                "    OutputLocation={\n",
                "        'Bucket': 'project-lf',\n",
                "        'Key': 'dataquality/'\n",
                "    },\n",
                "    RoleArn='arn:aws:iam::618572314333:role/DataBrew-Data-Quality-Workflows-DataAccessRole-ZRAZ5RGWCQW0'\n",
                ")\n",
                "\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "\n",
                "databrew = boto3.client('databrew', region_name='us-east-1')\n",
                "bucket = 'test-lf-wm'\n",
                "key = 'etl/landing/claim_db/provider/full/202305221136-provider-full.csv'\n",
                "dataset_name = 'test-landing-claimdb-provider'\n",
                "\n",
                "response = databrew.create_dataset(\n",
                "    Name=\"202305221132-policyholder-full\",\n",
                "    Format='GLUE_TABLE',\n",
                "    Input={\n",
                "        'DatabaseInputDefinition': {\n",
                "            'DatabaseName': 'test-release-3',\n",
                "            'TableName': dataset_name\n",
                "        }\n",
                "    }\n",
                ") \n",
                "\n",
                "\n",
                "print(response)\n",
                "\n",
                "\n",
                "# ConflictException if dataset already exists."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Start Run 1\n",
                "\n",
                "### Load data.\n",
                "Inputs needed: \n",
                "\n",
                "Process: `stage_x_into_y`\n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Create data catalog.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Connect dataset to data catalog.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Run profile job.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Get profile job results.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Create data quality job.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Run data quality job.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "### Get data quality job results.\n",
                "Inputs needed: \n",
                "\n",
                "Process: \n",
                "\n",
                "Output: `df` and `full path`\n",
                "\n",
                "\n",
                "## Start Run 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "sys.path.append('/home/glue_user/project_lf/ETL-TDD')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {},
            "outputs": [],
            "source": [
                "from automation.databrew import DataBrew\n",
                "\n",
                "databrew = DataBrew('us-east-1')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SLF4J: Class path contains multiple SLF4J bindings.\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
                        "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
                        "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
                        "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
                        "log4j:WARN Please initialize the log4j system properly.\n",
                        "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "getInstance(**kwargs={}) called\n",
                        "kwargs is {'region_name': 'us-east-1'}\n",
                        "endpoint is s3(https://s3.amazonaws.com)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "import stage_claim_into_raw\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
                "df, path = stage_claim_into_raw.run(spark, 'test-lf-wm')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 132,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'s3://test-lf-wm/etl/raw/claim_db/claim/full/202306041621/'"
                        ]
                    },
                    "execution_count": 132,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pathO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_result(path):\n",
                "    \"\"\"\n",
                "    Parse S3 path into bucket and key.\n",
                "    \"\"\"\n",
                "    path = path.lower().strip()\n",
                "    components = path.split('/')\n",
                "    return {\n",
                "        'path' : path,\n",
                "        'bucket' : components[2],\n",
                "        'tier' : components[4],\n",
                "        'table' : components[6],\n",
                "        'key' : '/'.join(components[4:][:-1]),\n",
                "        'env': components[2].split('-')[0]\n",
                "    }\n",
                "    \n",
                "pathO = path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# GLUE AUTOMATION UTILITIES\n",
                "# ----------------------------------------------------------------------------\n",
                "def get_job_name(path):\n",
                "    \"\"\"\n",
                "    Get the job name for the DataBrew profile job.\n",
                "    \"\"\"\n",
                "    return f\"{path['env']}-{path['tier']}-{path['table']}\"\n",
                "\n",
                "def get_table_name(path):\n",
                "    \"\"\"\n",
                "    Get the table name for the DataBrew profile job.\n",
                "    \"\"\"\n",
                "    return f\"{path['tier']}{path['table']}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('test-raw-claim', 'rawclaim')"
                        ]
                    },
                    "execution_count": 56,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_job_name(path), get_table_name(path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 115,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Job 'profile-test-raw-claim' already exists\n"
                    ]
                }
            ],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# CREATE PROFILE JOB\n",
                "# ----------------------------------------------------------------------------\n",
                "try: \n",
                "    databrew.create_profile_job(get_job_name(path),\n",
                "                            f\"profile-{get_job_name(path)}\", \n",
                "                            bucket='test-lf-wm', \n",
                "                            key='dataquality')\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' already exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 116,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset 'test-raw' already exists.\n"
                    ]
                }
            ],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# CREATE DATASET FOR DATA CATALOG ITEM\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    job = get_job_name(path)\n",
                "    table = get_table_name(path)\n",
                "    databrew.create_dataset(name=job, \n",
                "                        table=table, \n",
                "                        database='test-release-3', \n",
                "                        CatalogId='618572314333')\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Dataset '{name}' already exists.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 121,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Job 'profile-test-raw-claim' is already running\n"
                    ]
                }
            ],
            "source": [
                "# ----------------------------------------------------------------------------\n",
                "# RUN PROFILE JOB\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    databrew.start_profile_job(f\"profile-{get_job_name(path)}\")\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' is already running\")\n",
                "except databrew.client.exceptions.ResourceNotFoundException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' does not exist\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 124,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_profile_object_path(job_name):\n",
                "    \"\"\"\n",
                "    Get the profile object for a given job name.\n",
                "    \"\"\"\n",
                "    latest_run = databrew.client.list_job_runs(Name=job_name)['JobRuns'][0]\n",
                "    # Get location information.\n",
                "    location = latest_run['Outputs'][0]['Location']\n",
                "    # Get path\n",
                "    bucket, key = location['Bucket'], location['Key']\n",
                "    return bucket,key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_profile_link(dataset):\n",
                "    return f\"https://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset={dataset}&tab=profile-overview\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 127,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('test-lf-wm',\n",
                            " 'dataquality/test-raw-claim_672988221f4998bda2c49474bb323ba1898599bb7895830747936916f9269bb3.json')"
                        ]
                    },
                    "execution_count": 127,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_profile_object_path(job_name=f\"profile-test-raw-claim\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "    \"Attempt\": 0,\n",
                        "    \"CompletedOn\": \"2023-06-04 08:00:20.610000+00:00\",\n",
                        "    \"DatasetName\": \"test-raw-claim\",\n",
                        "    \"ExecutionTime\": 135,\n",
                        "    \"JobName\": \"profile-test-raw-claim\",\n",
                        "    \"RunId\": \"db_e8f23d26addbdb1da6258f6a086e99bd3724deded967ad1deef4682849b04386\",\n",
                        "    \"State\": \"SUCCEEDED\",\n",
                        "    \"LogSubscription\": \"DISABLE\",\n",
                        "    \"Outputs\": [\n",
                        "        {\n",
                        "            \"Format\": \"JSON\",\n",
                        "            \"Location\": {\n",
                        "                \"Bucket\": \"test-lf-wm\",\n",
                        "                \"Key\": \"dataquality/test-raw-claim_e8f23d26addbdb1da6258f6a086e99bd3724deded967ad1deef4682849b04386.json\",\n",
                        "                \"BucketOwner\": \"618572314333\"\n",
                        "            },\n",
                        "            \"Overwrite\": false\n",
                        "        }\n",
                        "    ],\n",
                        "    \"StartedBy\": \"arn:aws:iam::618572314333:user/Lead\",\n",
                        "    \"StartedOn\": \"2023-06-04 07:57:54.526000+00:00\",\n",
                        "    \"JobSample\": {\n",
                        "        \"Mode\": \"CUSTOM_ROWS\",\n",
                        "        \"Size\": 20000\n",
                        "    }\n",
                        "}\n",
                        "SUCCEEDED\n"
                    ]
                }
            ],
            "source": [
                "for job in databrew.client.list_job_runs(Name=f\"profile-{get_job_name(path)}\")['JobRuns']:\n",
                "    finished_running = True\n",
                "    if job['State'] != 'SUCCEEDED':\n",
                "        finished_running = False\n",
                "    print(json.dumps(job, indent=4,default=str))\n",
                "    print(job['State'])\n",
                "    print(get_result(job['Outputs']['Location']['Key']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 107,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'GlueDataBrew' object has no attribute 'describe_profile_job_run'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fetch the job results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m databrew\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mdescribe_profile_job_run\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:861\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39mif\u001b[39;00m event_response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 861\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    862\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m )\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'GlueDataBrew' object has no attribute 'describe_profile_job_run'"
                    ]
                }
            ],
            "source": [
                "# Fetch the job results\n",
                "results = databrew.client.describe_profile_job_run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "\n",
                "# Create a CloudWatch Logs client\n",
                "cloudwatch_logs = boto3.client('logs', region_name='us-east-1')\n",
                "\n",
                "# Get the log events\n",
                "response = cloudwatch_logs.get_log_events(\n",
                "    logGroupName='/aws-glue/crawlers',\n",
                "    logStreamName='test-raw',\n",
                ")\n",
                "\n",
                "# Print the log events\n",
                "for event in response['events']:\n",
                "    print(event['timestamp'], event['message'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 134,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'s3://test-lf-wm/etl/raw'"
                        ]
                    },
                    "execution_count": 134,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "write_path = 's3://test-lf-wm/etl/raw/claim_db/claim/full/202306041621/'\n",
                "\n",
                "# extract up to claim_db\n",
                "path = write_path.split('/')\n",
                "path = '/'.join(path[:path.index('claim_db')])\n",
                "path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Couldn't create crawler. Here's why: AlreadyExistsException: 618572314333:test-raw already exists\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Crawler test-raw already exists.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Couldn't start crawler test-raw. Here's why: CrawlerRunningException: Crawler with name test-raw has already started\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Crawler test-raw is already running.\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 10\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 21\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 32\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 43\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 53\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 64\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 75\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 85\n",
                        "Crawler test-raw is STOPPING.\n",
                        "\t 96\n",
                        "Time to view result: 96.73635339736938\n"
                    ]
                }
            ],
            "source": [
                "from databrew import DataBrew\n",
                "from glue import GlueWrapper\n",
                "from time import time, sleep \n",
                "\n",
                "glue = GlueWrapper('us-east-1')\n",
                "databrew = DataBrew('us-east-1')\n",
                "\n",
                "path = 's3://test-lf-wm/etl/raw/claim_db/claim/full/202306041621/'\n",
                "\n",
                "# Create crawler to create catalog.\n",
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = f\"{path['env']}-{path['tier']}\"\n",
                "database_name = f\"{path['env']}-release-3\"\n",
                "s3_target_path = glue.create_crawler_path(path)\n",
                "\n",
                "\n",
                "# ----------------------------------------------------------------------------\n",
                "# CREATE AND START CRAWLER\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    glue.create_crawler(name, role, database_name, path['tier'], s3_target_path)\n",
                "except glue.glue_client.exceptions.AlreadyExistsException:\n",
                "    print(f\"Crawler {name} already exists.\")\n",
                "\n",
                "try:\n",
                "    # glue.start_crawler(name)\n",
                "    print(\"Pretending crawler was started.\")\n",
                "    print(f\"Crawler {name} started.\")\n",
                "except glue.glue_client.exceptions.CrawlerRunningException:\n",
                "    print(f\"Crawler {name} is already running.\")\n",
                "    \n",
                "# glue.start_crawler(name)\n",
                "# wait for crawler to finish by checking its reponse\n",
                "start = time()\n",
                "while glue.get_crawler(name)['State'] not in ['READY']:\n",
                "    print(f\"Crawler {name} is {glue.get_crawler(name)['State']}.\")\n",
                "    sleep(10)\n",
                "    print('\\t', int(time() - start))\n",
                "    if int(time() - start) > 60 * 3:\n",
                "        print(\"Crawler wait period timed out.\")\n",
                "        break\n",
                "finish = time()\n",
                "\n",
                "print(f\"Time to view result: {finish - start}\")\n",
                "\n",
                "# ----------------------------------------------------------------------------\n",
                "# CREATE A DATASET CONNECTED TO THE DATA CATALOG\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    job = get_job_name(path)\n",
                "    table = get_table_name(path)\n",
                "    databrew.create_dataset(name=job, \n",
                "                        table=table, \n",
                "                        database='test-release-3', \n",
                "                        CatalogId='618572314333')\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Dataset '{name}' already exists.\")\n",
                "\n",
                "# ----------------------------------------------------------------------------\n",
                "# CREATE A PROFILE JOB FOR THE DATASET\n",
                "# ----------------------------------------------------------------------------\n",
                "try: \n",
                "    databrew.create_profile_job(get_job_name(path),\n",
                "                            f\"profile-{get_job_name(path)}\", \n",
                "                            bucket='test-lf-wm', \n",
                "                            key='dataquality')\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' already exists\")\n",
                "\n",
                "\n",
                "# ----------------------------------------------------------------------------\n",
                "# RUN PROFILE JOB\n",
                "# ----------------------------------------------------------------------------\n",
                "try:\n",
                "    databrew.start_profile_job(f\"profile-{get_job_name(path)}\")\n",
                "except databrew.client.exceptions.ConflictException:\n",
                "    print(f\"Job 'profile-{get_job_name(path)}' is already running\")\n",
                "\n",
                "# ----------------------------------------------------------------------------\n",
                "# GET PROFILE PATH\n",
                "# ----------------------------------------------------------------------------\n",
                "bucket, key = get_profile_object_path(job_name=f\"profile-test-raw-claim\")\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Fetch the job results\n",
                "results = databrew.describe_profile_job_run(jobName=f\"profile-{get_job_name(path)}\", runId=latest_run_id)\n",
                "print(f\"Job results: {results}\")\n",
                "\n",
                "# ----------------------------------------------------------------------------\n",
                "# VALIDATE\n",
                "# ----------------------------------------------------------------------------\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(json.dumps(glue.get_crawler(name),indent=4, sort_keys=True, default=str))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# use json dumps for result \n",
                "print(json.dumps(result, indent=4, sort_keys=True, default=str))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "\n",
                "role = \"arn:aws:iam::618572314333:role/service-role/AWSGlueServiceRole-ETL\"\n",
                "name = \"boto3-dq-crawler\"\n",
                "database_name = \"data-quality\"\n",
                "s3_target_path = \"s3://project-lf/data_quality\"\n",
                "\n",
                "glue = GlueWrapper(boto3.client('glue', region_name='us-east-1'))\n",
                "\n",
                "try:\n",
                "    response = glue.create_crawler(name, role, database_name, '', s3_target_path)\n",
                "    print(response)\n",
                "except glue.glue_client.exceptions.AlreadyExistsException as e:\n",
                "    print(e)\n",
                "    \n",
                "if glue.get_crawler(name) == None:\n",
                "    print(\"Crawler does not exist.\")\n",
                "else:\n",
                "    print(\"Crawler exists.\")\n",
                "    \n",
                "try:\n",
                "    response = glue.start_crawler(name)\n",
                "    print(response)\n",
                "except glue.glue_client.exceptions.CrawlerRunningException as e:\n",
                "    print(e)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Link to profile report as part of logging.\n",
                "https://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=landing-claimdb-policyholder&tab=profile-overview\n",
                "\n",
                "`https://us-east-1.console.aws.amazon.com/databrew/home?region={region}#dataset-details?dataset={profile-job-name}&tab=profile-overview`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

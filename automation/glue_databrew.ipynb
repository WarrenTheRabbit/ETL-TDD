{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "# Import glue dependencies.\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# Add paths so that this notebook can be executed anywhere in the \n",
    "# workspace's folder structure.\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD\")\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD/automation\")\n",
    "\n",
    "# Import batch job's custom dependencies.\n",
    "from databrew import DataBrew\n",
    "from glue import GlueWrapper\n",
    "from etl.paths.components import Bucket\n",
    "\n",
    "# Import jobs.\n",
    "import stage_claim_into_raw\n",
    "import stage_policyholder_into_raw\n",
    "import stage_provider_into_raw\n",
    "import stage_claim_into_access\n",
    "import stage_policyholder_into_access\n",
    "import stage_provider_into_access\n",
    "import stage_location_into_optimised\n",
    "import stage_procedure_into_optimised\n",
    "import stage_policyholder_into_optimised\n",
    "import stage_provider_into_optimised\n",
    "import stage_date_into_optimised\n",
    "import stage_claim_into_optimised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialise spark session with minimal logging.\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Wrap low-level AWS clients in a high-level object oriented API that uses\n",
    "# S3 paths to create and coordinate AWS services.\n",
    "glue = GlueWrapper('us-east-1')\n",
    "databrew = DataBrew('us-east-1')\n",
    "\n",
    "# Define the bucket to use (TEST or PROD).\n",
    "env = Bucket.TEST\n",
    "\n",
    "# Define the sequential job batches.\n",
    "# Earlier batches must be completed before later batches.\n",
    "batch1 = [ stage_claim_into_raw, \n",
    "           stage_policyholder_into_raw, \n",
    "           stage_provider_into_raw ]\n",
    "\n",
    "batch2 = [ stage_claim_into_access, \n",
    "           stage_policyholder_into_access, \n",
    "           stage_provider_into_access ]\n",
    "\n",
    "batch3 = [ stage_location_into_optimised, \n",
    "           stage_procedure_into_optimised ]\n",
    "\n",
    "batch4 = [ stage_policyholder_into_optimised, \n",
    "           stage_provider_into_optimised, \n",
    "           stage_date_into_optimised ]\n",
    "\n",
    "batch5 = [ stage_claim_into_optimised ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything in the raw, access and optimised tiers\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('test-lf-wm')\n",
    "\n",
    "# ********************* RESET RAW *********************\n",
    "for obj in bucket.objects.filter(Prefix='etl/raw'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# **********************RESET ACCESS ******************\n",
    "for obj in bucket.objects.filter(Prefix='etl/access'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# ******************** RESET OPTIMISED ****************\n",
    "for obj in bucket.objects.filter(Prefix='etl/optimised'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch1:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-raw.\n",
      "Waiting for crawler to finish.........................\n",
      "Waiting for job to finish....................\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306052301/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306052301/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306052302/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch2:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-access.\n",
      "Waiting for crawler to finish............\n",
      "Waiting for job to finish........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/claim/full/202306052307/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/policyholder/full/202306052307/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/provider/full/202306052308/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Use data profile. #####################\n",
    "\n",
    "# databrew.get_dq_results(paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch3:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't delete crawler:\n",
      "                Crawler entry with name test-optimised does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-optimised.\n",
      "Waiting for crawler to finish..........\n",
      "Waiting for job to finish.........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/location_dim/full/202306052312/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-location-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/procedure_dim/full/202306052313/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-procedure-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/location_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/location_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch4:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-optimised.\n",
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/policyholder_dim/full/202306052317/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-policyholder-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/provider_dim/full/202306052317/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-provider-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/date_dim/full/202306052318/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-date-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/date_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/policyholder_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/procedure_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/provider_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch5:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-optimised.\n",
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish.......................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/claim_fact/full/202306052322/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-claim-fact&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

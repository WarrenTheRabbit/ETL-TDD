{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "# Import glue dependencies.\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# Add the project root so that this notebook can be executed anywhere in the \n",
    "# folder structure.\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD\")\n",
    "\n",
    "# Import batch job's custom dependencies.\n",
    "from databrew import DataBrew\n",
    "from glue import GlueWrapper\n",
    "from etl.paths.components import Bucket\n",
    "\n",
    "# Import jobs.\n",
    "import stage_claim_into_raw\n",
    "import stage_policyholder_into_raw\n",
    "import stage_provider_into_raw\n",
    "import stage_claim_into_access\n",
    "import stage_policyholder_into_access\n",
    "import stage_provider_into_access\n",
    "import stage_location_into_optimised\n",
    "import stage_procedure_into_optimised\n",
    "import stage_policyholder_into_optimised\n",
    "import stage_provider_into_optimised\n",
    "import stage_date_into_optimised\n",
    "import stage_claim_into_optimised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialise spark session with minimal logging.\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Wrap low-level AWS clients in a high-level object oriented API that uses\n",
    "# S3 paths to create and coordinate AWS services.\n",
    "glue = GlueWrapper('us-east-1')\n",
    "databrew = DataBrew('us-east-1')\n",
    "\n",
    "# Define the bucket to use (TEST or PROD).\n",
    "env = Bucket.TEST\n",
    "\n",
    "# Define the sequential job batches.\n",
    "# Earlier batches must be completed before later batches.\n",
    "batch1 = [stage_claim_into_raw, stage_policyholder_into_raw, stage_provider_into_raw]\n",
    "batch2 = [stage_claim_into_access, stage_policyholder_into_access, stage_provider_into_access]\n",
    "batch3 = [stage_location_into_optimised, stage_procedure_into_optimised]\n",
    "batch4 = [stage_policyholder_into_optimised, stage_provider_into_optimised, stage_date_into_optimised]\n",
    "batch5 = [stage_claim_into_optimised]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything in the raw, access and optimised tiers\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('test-lf-wm')\n",
    "\n",
    "# ********************* RESET RAW *********************\n",
    "for obj in bucket.objects.filter(Prefix='etl/raw'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# **********************RESET ACCESS ******************\n",
    "for obj in bucket.objects.filter(Prefix='etl/access'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# ******************** RESET OPTIMISED ****************\n",
    "for obj in bucket.objects.filter(Prefix='etl/optimised'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch1:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s3://test-lf-wm/etl/access/claim_db/claim/full/202306052244/',\n",
       "       's3://test-lf-wm/etl/access/claim_db/policyholder/full/202306052244/',\n",
       "       's3://test-lf-wm/etl/access/claim_db/provider/full/202306052244/'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't delete crawler:\n",
      "                Crawler entry with name test-raw does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-raw.\n",
      "Waiting for crawler to finish.............\n",
      "Waiting for job to finish.......................\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306052239/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306052239/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306052240/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch2:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't delete crawler:\n",
      "                Crawler entry with name test-access does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-access.\n",
      "Waiting for crawler to finish..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                AWS Glue table access-claim was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-access-claim wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-access-claim wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table access-policyholder was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-access-policyholder wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-access-policyholder wasn't found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish......................\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/claim/full/202306052244/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/policyholder/full/202306052244/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/provider/full/202306052244/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "# Print links.\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceNotFoundException",
     "evalue": "An error occurred (ResourceNotFoundException) when calling the ListJobRuns operation: The job test-access-claim wasn't found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFoundException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m############## Use data profile. #####################\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m databrew\u001b[39m.\u001b[39;49mget_dq_results(paths[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/project_lf/ETL-TDD/automation/databrew.py:173\u001b[0m, in \u001b[0;36mDataBrew.get_dq_results\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dq_results\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m    167\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m    Get the data quality results from the S3 bucket.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m    The data quality results can be used to validate a transformation.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     bucket,key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_path_to_profile_file(path)\n\u001b[1;32m    175\u001b[0m     s3_resource \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mresource(\u001b[39m'\u001b[39m\u001b[39ms3\u001b[39m\u001b[39m'\u001b[39m, region_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mus-east-1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m     s3_object \u001b[39m=\u001b[39m s3_resource\u001b[39m.\u001b[39mObject(bucket, key)\n",
      "File \u001b[0;32m~/project_lf/ETL-TDD/automation/databrew.py:148\u001b[0m, in \u001b[0;36mDataBrew.get_path_to_profile_file\u001b[0;34m(self, path, key)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_path_to_profile_file\u001b[39m(\u001b[39mself\u001b[39m,path, key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataquality\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    140\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    Get the profile object for a given job name.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     latest_run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mlist_job_runs(Name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_job_name(path))[\u001b[39m'\u001b[39m\u001b[39mJobRuns\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    149\u001b[0m     \u001b[39m# Get location information.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     location \u001b[39m=\u001b[39m latest_run[\u001b[39m'\u001b[39m\u001b[39mOutputs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mLocation\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:514\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:938\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceNotFoundException\u001b[0m: An error occurred (ResourceNotFoundException) when calling the ListJobRuns operation: The job test-access-claim wasn't found."
     ]
    }
   ],
   "source": [
    "############## Use data profile. #####################\n",
    "\n",
    "# databrew.get_dq_results(paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch3:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-raw.\n",
      "Waiting for crawler to finish.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_claim was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-claim wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-claim wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_policyholder was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-policyholder wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-policyholder wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_provider was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-provider wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-provider wasn't found.\n",
      "Couldn't get job state:\n",
      "\tThe job test-raw-provider wasn't found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306052137/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306052138/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306052138/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch4:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-raw.\n",
      "Waiting for crawler to finish.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_claim was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-claim wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-claim wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_policyholder was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-policyholder wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-policyholder wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_provider was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-provider wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-provider wasn't found.\n",
      "Couldn't get job state:\n",
      "\tThe job test-raw-provider wasn't found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306052137/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306052138/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306052138/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch5:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-raw.\n",
      "Waiting for crawler to finish.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_claim was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-claim wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-claim wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_policyholder was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-policyholder wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-policyholder wasn't found.\n",
      "Couldn't create dataset:\n",
      "                AWS Glue table raw_provider was not found in database test-release-3 in catalogId 618572314333.\n",
      "Couldn't create data profile job:\n",
      "                Dataset test-raw-provider wasn't found\n",
      "Couldn't start profile job run:\n",
      "\tThe job test-raw-provider wasn't found.\n",
      "Couldn't get job state:\n",
      "\tThe job test-raw-provider wasn't found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306052137/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306052138/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306052138/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Redshift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "# Import glue dependencies.\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# Add paths so that this notebook can be executed anywhere in the \n",
    "# workspace's folder structure.\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD\")\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD/automation\")\n",
    "\n",
    "# Import batch job's custom dependencies.\n",
    "from automation.databrew import DataBrew\n",
    "from automation.glue import Glue\n",
    "from automation.redshift import Redshift\n",
    "from etl.paths.components import Bucket\n",
    "from automation.batch import Batch\n",
    "\n",
    "# Import jobs.\n",
    "import stage_claim_into_raw\n",
    "import stage_policyholder_into_raw\n",
    "import stage_provider_into_raw\n",
    "import stage_claim_into_access\n",
    "import stage_policyholder_into_access\n",
    "import stage_provider_into_access\n",
    "import stage_location_into_optimised\n",
    "import stage_procedure_into_optimised\n",
    "import stage_policyholder_into_optimised\n",
    "import stage_provider_into_optimised\n",
    "import stage_date_into_optimised\n",
    "import stage_claim_into_optimised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialise spark session with minimal logging.\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Wrap low-level AWS clients in a high-level object oriented API that uses\n",
    "# S3 paths to create and coordinate AWS services.\n",
    "glue = Glue('ap-southeast-2')\n",
    "databrew = DataBrew('ap-southeast-2')\n",
    "\n",
    "# Initialise Redshift helper.\n",
    "redshift = Redshift()\n",
    "\n",
    "# Define the bucket to use (TEST or PROD).\n",
    "env = 'test-project-wm'\n",
    "\n",
    "# Define the sequential job batches.\n",
    "# Earlier batches must be completed before later batches.\n",
    "\n",
    "config = { \n",
    "            \"spark\": spark,\n",
    "            \"env\": env\n",
    "         }\n",
    "\n",
    "batch1 = Batch([\n",
    "                  stage_claim_into_raw, \n",
    "                  stage_policyholder_into_raw, \n",
    "                  stage_provider_into_raw\n",
    "               ], \n",
    "               **config)\n",
    "\n",
    "batch2 = Batch([\n",
    "                  stage_claim_into_access, \n",
    "                  stage_policyholder_into_access, \n",
    "                  stage_provider_into_access\n",
    "               ], \n",
    "               **config)\n",
    "\n",
    "batch3 = Batch([\n",
    "                  stage_location_into_optimised, \n",
    "                  stage_procedure_into_optimised\n",
    "               ], \n",
    "               **config)\n",
    "\n",
    "batch4 = Batch([\n",
    "                  stage_policyholder_into_optimised, \n",
    "                  stage_provider_into_optimised, \n",
    "                  stage_date_into_optimised\n",
    "               ], \n",
    "               **config)\n",
    "\n",
    "batch5 = Batch([stage_claim_into_optimised], \n",
    "               **config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything in the raw, access and optimised tiers\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(str(env))\n",
    "\n",
    "# ********************* RESET RAW *********************\n",
    "for obj in bucket.objects.filter(Prefix='etl/raw'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# **********************RESET ACCESS ******************\n",
    "for obj in bucket.objects.filter(Prefix='etl/access'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# ******************** RESET OPTIMISED ****************\n",
    "for obj in bucket.objects.filter(Prefix='etl/optimised'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-ap') etl/landing/claim_db/claim/full [s3.ObjectSummary(bucket_name='test-lf-ap', key='etl/landing/claim_db/claim/full/202305211851-claim-full.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-ap') etl/landing/claim_db/policyholder/full [s3.ObjectSummary(bucket_name='test-lf-ap', key='etl/landing/claim_db/policyholder/full/202305221132-policyholder-full.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-ap') etl/landing/claim_db/provider/full [s3.ObjectSummary(bucket_name='test-lf-ap', key='etl/landing/claim_db/provider/full/202305221136-provider-full.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "batch1.run()\n",
    "redshift.get_all_redshift_loads(batch1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s3://test-lf-ap/etl/raw/claim_db/claim/full/202306062111/',\n",
       "       's3://test-lf-ap/etl/raw/claim_db/policyholder/full/202306062111/',\n",
       "       's3://test-lf-ap/etl/raw/claim_db/provider/full/202306062111/'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler 'test-raw'.\n",
      "Waiting for crawler to finish..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-raw-claim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-raw-claim already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-raw-policyholder already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-raw-policyholder already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-raw-provider already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-raw-provider already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish.........................\n",
      "You can view the data profile for s3://test-lf-ap/etl/raw/claim_db/claim/full/202306062111/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/raw/claim_db/policyholder/full/202306062111/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/raw/claim_db/provider/full/202306062111/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "representative_path = batch1.paths[0]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(representative_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(batch1.paths)\n",
    "glue.start_crawler(representative_path)\n",
    "\n",
    "glue.wait_for_crawler(representative_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in batch1.paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)        \n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(representative_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in batch1.paths:\n",
    "    databrew.show_data_profile_link(path)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "batch2.run()\n",
    "redshift.get_all_redshift_loads(batch2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler 'test-access'.\n",
      "Waiting for crawler to finish............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-access-claim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-access-claim already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-access-policyholder already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-access-policyholder already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-access-provider already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-access-provider already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish....................\n",
      "You can view the data profile for s3://test-lf-ap/etl/access/claim_db/claim/full/202306062115/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/access/claim_db/policyholder/full/202306062115/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/access/claim_db/provider/full/202306062115/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "representatitve_path = batch2.paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(representatitve_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(batch2.paths)\n",
    "glue.start_crawler(representatitve_path)\n",
    "\n",
    "glue.wait_for_crawler(representatitve_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in batch2.paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(representatitve_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in batch2.paths:\n",
    "    databrew.show_data_profile_link(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Use data profile. #####################\n",
    "\n",
    "# databrew.get_dq_results(paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "batch3.run()\n",
    "redshift.get_all_redshift_loads(batch3.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler 'test-optimised'.\n",
      "Waiting for crawler to finish...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-location-dim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-optimised-location-dim already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-procedure-dim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-optimised-procedure-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish....................\n",
      "You can view the data profile for s3://test-lf-ap/etl/optimised/location_dim/full/202306062118/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-location-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/optimised/procedure_dim/full/202306062118/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-procedure-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "representative_path = batch3.paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(representative_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(batch3.paths)\n",
    "glue.start_crawler(representative_path)\n",
    "\n",
    "glue.wait_for_crawler(representative_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in batch3.paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(representative_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in batch3.paths:\n",
    "    databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "batch4.run()\n",
    "redshift.get_all_redshift_loads(batch4.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler 'test-optimised'.\n",
      "Waiting for crawler to finish............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-policyholder-dim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-optimised-policyholder-dim already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-provider-dim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-optimised-provider-dim already exists.\n",
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-date-dim already exists.\n",
      "Couldn't create data profile job:\n",
      "                The job test-optimised-date-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish...............................\n",
      "You can view the data profile for s3://test-lf-ap/etl/optimised/policyholder_dim/full/202306062121/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-policyholder-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/optimised/provider_dim/full/202306062121/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-provider-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-ap/etl/optimised/date_dim/full/202306062121/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-date-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "representative_path = batch4.paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(representative_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(batch4.paths)\n",
    "glue.start_crawler(representative_path)\n",
    "\n",
    "glue.wait_for_crawler(representative_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in batch4.paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(representative_path)\n",
    "\n",
    "for path in batch4.paths:\n",
    "    databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "batch5.run()\n",
    "redshift.get_all_redshift_loads(batch5.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler 'test-optimised'.\n",
      "Waiting for crawler to finish.........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-claim-fact already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create data profile job:\n",
      "                The job test-optimised-claim-fact already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish...........................\n",
      "You can view the data profile for s3://test-lf-ap/etl/optimised/claim_fact/full/202306062125/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-claim-fact&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "representative_path = batch5.paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(representative_path)\n",
    "glue.create_crawler(batch5.paths)\n",
    "glue.start_crawler(representative_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(representative_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in batch5.paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(representative_path)\n",
    "\n",
    "for path in batch5.paths:\n",
    "    databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift - Initial Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(\"/home/glue_user/project_lf/ETL-TDD/.env\")\n",
    "\n",
    "for key in os.environ.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `copy` statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY location_dim\n",
      "    FROM 's3://test-lf-ap/etl/optimised/location_dim/full/202306062118/'\n",
      "    IAM_ROLE 'arn:aws:iam::618572314333:role/service-role/AmazonRedshift-CommandsAccessRole-20230513T114656'\n",
      "    FORMAT AS PARQUET; \n",
      "\n",
      "COPY procedure_dim\n",
      "    FROM 's3://test-lf-ap/etl/optimised/procedure_dim/full/202306062118/'\n",
      "    IAM_ROLE 'arn:aws:iam::618572314333:role/service-role/AmazonRedshift-CommandsAccessRole-20230513T114656'\n",
      "    FORMAT AS PARQUET; \n",
      "\n",
      "COPY policyholder_dim\n",
      "    FROM 's3://test-lf-ap/etl/optimised/policyholder_dim/full/202306062121/'\n",
      "    IAM_ROLE 'arn:aws:iam::618572314333:role/service-role/AmazonRedshift-CommandsAccessRole-20230513T114656'\n",
      "    FORMAT AS PARQUET; \n",
      "\n",
      "COPY provider_dim\n",
      "    FROM 's3://test-lf-ap/etl/optimised/provider_dim/full/202306062121/'\n",
      "    IAM_ROLE 'arn:aws:iam::618572314333:role/service-role/AmazonRedshift-CommandsAccessRole-20230513T114656'\n",
      "    FORMAT AS PARQUET; \n",
      "\n",
      "COPY date_dim\n",
      "    FROM 's3://test-lf-ap/etl/optimised/date_dim/full/202306062121/'\n",
      "    IAM_ROLE 'arn:aws:iam::618572314333:role/service-role/AmazonRedshift-CommandsAccessRole-20230513T114656'\n",
      "    FORMAT AS PARQUET; \n",
      "\n",
      "COPY claim_fact\n",
      "    FROM 's3://test-lf-ap/etl/optimised/claim_fact/full/202306062125/'\n",
      "    IAM_ROLE 'arn:aws:iam::618572314333:role/service-role/AmazonRedshift-CommandsAccessRole-20230513T114656'\n",
      "    FORMAT AS PARQUET; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for statement in redshift.get_copy_commands():\n",
    "    print(statement, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------+----------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|location_key|             address|              street|postcode|    suburb|track_hash|    record_start_ts|      record_end_ts|record_active_flag|       record_upd_ts|    record_insert_ts|\n",
      "+------------+--------------------+--------------------+--------+----------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|           0|Level 2 85 Rhonda...|Level 2 85 Rhonda...|    2987| Jimmyfurt|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-06 11:36:...|2023-06-06 11:36:...|\n",
      "+------------+--------------------+--------------------+--------+----------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- location_key: long (nullable = false)\n",
      " |-- address: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- suburb: string (nullable = true)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n",
      "StructType([StructField('location_key', LongType(), False), StructField('address', StringType(), True), StructField('street', StringType(), True), StructField('postcode', StringType(), True), StructField('suburb', StringType(), True), StructField('track_hash', LongType(), True), StructField('record_start_ts', StringType(), False), StructField('record_end_ts', StringType(), False), StructField('record_active_flag', ShortType(), False), StructField('record_upd_ts', StringType(), False), StructField('record_insert_ts', StringType(), False)])\n",
      "+---------+-------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|procedure|procedure_key|track_hash|    record_start_ts|      record_end_ts|record_active_flag|       record_upd_ts|    record_insert_ts|\n",
      "+---------+-------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|  CT Scan|            0|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-06 11:36:...|2023-06-06 11:36:...|\n",
      "+---------+-------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- procedure_key: long (nullable = false)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n",
      "StructType([StructField('procedure', StringType(), True), StructField('procedure_key', LongType(), False), StructField('track_hash', LongType(), True), StructField('record_start_ts', StringType(), False), StructField('record_end_ts', StringType(), False), StructField('record_active_flag', ShortType(), False), StructField('record_upd_ts', StringType(), False), StructField('record_insert_ts', StringType(), False)])\n",
      "+----------------+------------+----------------+----------+---------+------+---------------+--------------------+-------------+----------------------+---------------+-------------------+-----------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|policyholder_key|location_key|policy_holder_id|first_name|last_name|gender|   phone_number|       email_address|date_of_birth|insurance_plan_details|policy_standing|coverage_start_date|coverage_end_date|track_hash|    record_start_ts|      record_end_ts|record_active_flag|       record_upd_ts|    record_insert_ts|\n",
      "+----------------+------------+----------------+----------+---------+------+---------------+--------------------+-------------+----------------------+---------------+-------------------+-----------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|               0|          12|          885440|   Chelsea|    Nunez|Female|+61-2-2714-2787|hensontroy@exampl...|   2002-09-05|  Amet reprehenderi...|        Expired|         2015-07-25|       2015-10-24|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-06 11:36:...|2023-06-06 11:36:...|\n",
      "+----------------+------------+----------------+----------+---------+------+---------------+--------------------+-------------+----------------------+---------------+-------------------+-----------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- policyholder_key: long (nullable = false)\n",
      " |-- location_key: long (nullable = true)\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- email_address: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- insurance_plan_details: string (nullable = true)\n",
      " |-- policy_standing: string (nullable = true)\n",
      " |-- coverage_start_date: string (nullable = true)\n",
      " |-- coverage_end_date: string (nullable = true)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n",
      "StructType([StructField('policyholder_key', LongType(), False), StructField('location_key', LongType(), True), StructField('policy_holder_id', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('gender', StringType(), True), StructField('phone_number', StringType(), True), StructField('email_address', StringType(), True), StructField('date_of_birth', StringType(), True), StructField('insurance_plan_details', StringType(), True), StructField('policy_standing', StringType(), True), StructField('coverage_start_date', StringType(), True), StructField('coverage_end_date', StringType(), True), StructField('track_hash', LongType(), True), StructField('record_start_ts', StringType(), False), StructField('record_end_ts', StringType(), False), StructField('record_active_flag', ShortType(), False), StructField('record_upd_ts', StringType(), False), StructField('record_insert_ts', StringType(), False)])\n",
      "+------------+------------+-----------------------+-----------+-------------+---------------------+----------------------+-------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|provider_key|location_key|provider_license_number|provider_id|provider_name|provider_phone_number|provider_email_address|provider_type|track_hash|    record_start_ts|      record_end_ts|record_active_flag|       record_upd_ts|    record_insert_ts|\n",
      "+------------+------------+-----------------------+-----------+-------------+---------------------+----------------------+-------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|           0|           3|               82002543|     558433| Cox and Sons|      +61 2 4485 8099|  rmcdowell@example...|   Pharmacist|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-06 11:36:...|2023-06-06 11:36:...|\n",
      "+------------+------------+-----------------------+-----------+-------------+---------------------+----------------------+-------------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- provider_key: long (nullable = false)\n",
      " |-- location_key: long (nullable = true)\n",
      " |-- provider_license_number: string (nullable = true)\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_phone_number: string (nullable = true)\n",
      " |-- provider_email_address: string (nullable = true)\n",
      " |-- provider_type: string (nullable = true)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n",
      "StructType([StructField('provider_key', LongType(), False), StructField('location_key', LongType(), True), StructField('provider_license_number', StringType(), True), StructField('provider_id', IntegerType(), True), StructField('provider_name', StringType(), True), StructField('provider_phone_number', StringType(), True), StructField('provider_email_address', StringType(), True), StructField('provider_type', StringType(), True), StructField('track_hash', LongType(), True), StructField('record_start_ts', StringType(), False), StructField('record_end_ts', StringType(), False), StructField('record_active_flag', ShortType(), False), StructField('record_upd_ts', StringType(), False), StructField('record_insert_ts', StringType(), False)])\n",
      "+--------+----------+-----------+--------+------------+-----------+------------+-----+----------+-------+----+----------+----------+----------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|date_key|      date|day_of_week|day_name|day_of_month|day_of_year|week_of_year|month|month_name|quarter|year|is_weekend|is_weekday|is_holiday|track_hash|    record_start_ts|      record_end_ts|record_active_flag|       record_upd_ts|    record_insert_ts|\n",
      "+--------+----------+-----------+--------+------------+-----------+------------+-----+----------+-------+----+----------+----------+----------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|20000101|2000-01-01|          6|Saturday|           1|          1|          52|    1|   January|      1|2000|      true|     false|      true|      null|1970-01-01 00:00:00|2999-12-31 00:00:00|                 1|2023-06-06 11:36:...|2023-06-06 11:36:...|\n",
      "+--------+----------+-----------+--------+------------+-----------+------------+-----+----------+-------+----+----------+----------+----------+----------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- date_key: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_name: string (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n",
      "StructType([StructField('date_key', IntegerType(), True), StructField('date', StringType(), True), StructField('day_of_week', IntegerType(), True), StructField('day_name', StringType(), True), StructField('day_of_month', IntegerType(), True), StructField('day_of_year', IntegerType(), True), StructField('week_of_year', IntegerType(), True), StructField('month', IntegerType(), True), StructField('month_name', StringType(), True), StructField('quarter', IntegerType(), True), StructField('year', IntegerType(), True), StructField('is_weekend', BooleanType(), True), StructField('is_weekday', BooleanType(), True), StructField('is_holiday', BooleanType(), True), StructField('track_hash', LongType(), True), StructField('record_start_ts', StringType(), False), StructField('record_end_ts', StringType(), False), StructField('record_active_flag', ShortType(), False), StructField('record_upd_ts', StringType(), False), StructField('record_insert_ts', StringType(), False)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------------+-------------+--------+--------------------+-------------+-------------+------+-------------+\n",
      "|claim_id|provider_key|policyholder_key|procedure_key|date_key|total_procedure_cost|medibank_pays|medicare_pays|excess|out_of_pocket|\n",
      "+--------+------------+----------------+-------------+--------+--------------------+-------------+-------------+------+-------------+\n",
      "|  154100|           1|               7|            4|20220717|             2295.05|      1377.03|       459.01|229.51|        229.5|\n",
      "+--------+------------+----------------+-------------+--------+--------------------+-------------+-------------+------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- claim_id: integer (nullable = true)\n",
      " |-- provider_key: long (nullable = true)\n",
      " |-- policyholder_key: long (nullable = true)\n",
      " |-- procedure_key: long (nullable = true)\n",
      " |-- date_key: integer (nullable = true)\n",
      " |-- total_procedure_cost: float (nullable = true)\n",
      " |-- medibank_pays: float (nullable = true)\n",
      " |-- medicare_pays: float (nullable = true)\n",
      " |-- excess: float (nullable = true)\n",
      " |-- out_of_pocket: float (nullable = true)\n",
      "\n",
      "StructType([StructField('claim_id', IntegerType(), True), StructField('provider_key', LongType(), True), StructField('policyholder_key', LongType(), True), StructField('procedure_key', LongType(), True), StructField('date_key', IntegerType(), True), StructField('total_procedure_cost', FloatType(), True), StructField('medibank_pays', FloatType(), True), StructField('medicare_pays', FloatType(), True), StructField('excess', FloatType(), True), StructField('out_of_pocket', FloatType(), True)])\n"
     ]
    }
   ],
   "source": [
    "for df in [df for df,_ in redshift.loads]:\n",
    "    df.show(1, truncate=True)\n",
    "    df.printSchema()\n",
    "    print(df.schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databrew import DataBrew\n",
    "from glue import GlueWrapper\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD\")\n",
    "\n",
    "glue = GlueWrapper('us-east-1')\n",
    "databrew = DataBrew('us-east-1')\n",
    "\n",
    "# Run 1\n",
    "import stage_claim_into_raw\n",
    "import stage_policyholder_into_raw\n",
    "import stage_provider_into_raw\n",
    "\n",
    "# Run 2\n",
    "import stage_claim_into_access\n",
    "import stage_policyholder_into_access\n",
    "import stage_provider_into_access\n",
    "\n",
    "# Run 3\n",
    "import stage_location_into_optimised\n",
    "import stage_procedure_into_optimised\n",
    "\n",
    "# Run 4\n",
    "import stage_policyholder_into_optimised\n",
    "import stage_provider_into_optimised\n",
    "\n",
    "\n",
    "# Run 5\n",
    "import stage_claim_into_optimised\n",
    "\n",
    "\n",
    "# import glue dependencies\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from etl.paths.components import Bucket\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialise spark session\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything in the raw, access and optimised tiers\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('test-lf-wm')\n",
    "\n",
    "# ********************* RESET RAW *********************\n",
    "for obj in bucket.objects.filter(Prefix='etl/raw'):\n",
    "    # s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# **********************RESET ACCESS ******************\n",
    "for obj in bucket.objects.filter(Prefix='etl/access'):\n",
    "    # s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# ******************** RESET OPTIMISED ****************\n",
    "for obj in bucket.objects.filter(Prefix='etl/optimised'):\n",
    "    # s3.Object(bucket.name,obj.key).delete()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "raw_df, path = stage_claim_into_raw.run(spark, Bucket.TEST)\n",
    "outputs.append( (raw_df, path) )\n",
    "raw_df, path = stage_policyholder_into_raw.run(spark, Bucket.TEST)\n",
    "outputs.append( (raw_df, path) )\n",
    "raw_df, path = stage_provider_into_raw.run(spark, Bucket.TEST)\n",
    "outputs.append( (raw_df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flatten the outputs with numpy\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- claim_id: integer (nullable = true)\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- date_of_service: string (nullable = true)\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- total_procedure_cost: float (nullable = true)\n",
      " |-- medibank_pays: float (nullable = true)\n",
      " |-- medicare_pays: float (nullable = true)\n",
      " |-- excess: float (nullable = true)\n",
      " |-- out_of_pocket: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- email_address: string (nullable = true)\n",
      " |-- insurance_plan_details: string (nullable = true)\n",
      " |-- coverage_start_date: string (nullable = true)\n",
      " |-- coverage_end_date: string (nullable = true)\n",
      " |-- policy_standing: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_address: string (nullable = true)\n",
      " |-- provider_phone_number: string (nullable = true)\n",
      " |-- provider_email_address: string (nullable = true)\n",
      " |-- provider_type: string (nullable = true)\n",
      " |-- provider_license_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create crawler. 618572314333:test-raw already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler not running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset. Dataset test-raw-claim already exists.\n",
      "Couldn't create data profile job. The job test-raw-claim already exists.\n",
      "Couldn't create dataset. Dataset test-raw-provider already exists.\n",
      "Couldn't create data profile job. The job test-raw-provider already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish.........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306051005/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306051005/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306051006/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.create_crawler(example_path)\n",
    "\n",
    "# glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "access_df, path = stage_claim_into_access.run(spark, Bucket.TEST)\n",
    "outputs.append( (access_df, path) )\n",
    "access_df, path = stage_policyholder_into_access.run(spark, Bucket.TEST)\n",
    "outputs.append( (access_df, path) )\n",
    "access_df, path = stage_provider_into_access.run(spark, Bucket.TEST)\n",
    "outputs.append( (access_df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flatten the outputs with numpy\n",
    "access_dfs = numpy.array(outputs).flatten()[::2]\n",
    "access_paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- claim_id: integer (nullable = true)\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- date_of_service: string (nullable = true)\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- total_procedure_cost: float (nullable = true)\n",
      " |-- medibank_pays: float (nullable = true)\n",
      " |-- medicare_pays: float (nullable = true)\n",
      " |-- excess: float (nullable = true)\n",
      " |-- out_of_pocket: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- email_address: string (nullable = true)\n",
      " |-- insurance_plan_details: string (nullable = true)\n",
      " |-- coverage_start_date: string (nullable = true)\n",
      " |-- coverage_end_date: string (nullable = true)\n",
      " |-- policy_standing: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_address: string (nullable = true)\n",
      " |-- provider_phone_number: string (nullable = true)\n",
      " |-- provider_email_address: string (nullable = true)\n",
      " |-- provider_type: string (nullable = true)\n",
      " |-- provider_license_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in access_dfs:\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish...........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/claim/full/202306051354/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/policyholder/full/202306051355/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/provider/full/202306051355/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = access_paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.create_crawler(example_path)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in access_paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "# Print links.\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Use data profile. #####################\n",
    "\n",
    "databrew.get_dq_results(paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "kwargs is {'region_name': 'us-east-1'}\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run3 = []\n",
    "df, path = stage_location_into_optimised.run(spark,Bucket.TEST)\n",
    "run3.append( (df, path) )\n",
    "df, path = stage_procedure_into_optimised.run(spark,Bucket.TEST)\n",
    "run3.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['s3://test-lf-wm/etl/optimised/location_dim/full/202306051807/',\n",
       "        's3://test-lf-wm/etl/optimised/procedure_dim/full/202306051808/'],\n",
       "       dtype=object),\n",
       " array([DataFrame[location_key: bigint, address: string, street: string, postcode: string, suburb: string, track_hash: bigint, record_start_ts: timestamp, record_end_ts: timestamp, record_active_flag: smallint, record_upd_ts: timestamp, record_insert_ts: timestamp],\n",
       "        DataFrame[procedure: string, procedure_key: bigint, track_hash: bigint, record_start_ts: timestamp, record_end_ts: timestamp, record_active_flag: smallint, record_upd_ts: timestamp, record_insert_ts: timestamp]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run3_dfs = numpy.array(run3).flatten()[0::2]\n",
    "run3_paths = numpy.array(run3).flatten()[1::2]\n",
    "\n",
    "run3_paths, run3_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_key: long (nullable = false)\n",
      " |-- address: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- suburb: string (nullable = true)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: timestamp (nullable = true)\n",
      " |-- record_end_ts: timestamp (nullable = true)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: timestamp (nullable = false)\n",
      " |-- record_insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------------------------------------+------------------------------+--------+--------------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|location_key|address                                                       |street                        |postcode|suburb              |track_hash|record_start_ts    |record_end_ts      |record_active_flag|record_upd_ts          |record_insert_ts       |\n",
      "+------------+--------------------------------------------------------------+------------------------------+--------+--------------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|0           |Level 2 85 Rhonda Upper, Jimmyfurt, TAS, 2987                 |Level 2 85 Rhonda Upper       | 2987   | Jimmyfurt          |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:15:58.097|2023-06-05 08:15:58.097|\n",
      "|1           |Apt. 486 646 Michael Concourse, Lake Gabrielchester, VIC, 2901|Apt. 486 646 Michael Concourse| 2901   | Lake Gabrielchester|null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:15:58.097|2023-06-05 08:15:58.097|\n",
      "|2           |606 Hamilton Circle, Munozburgh, NT, 2957                     |606 Hamilton Circle           | 2957   | Munozburgh         |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:15:58.097|2023-06-05 08:15:58.097|\n",
      "|3           |695 Ryan Nook, St. Patrick, VIC, 2697                         |695 Ryan Nook                 | 2697   | St. Patrick        |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:15:58.097|2023-06-05 08:15:58.097|\n",
      "|4           |Flat 00 575 Erica Key, Elizabethbury, NT, 9818                |Flat 00 575 Erica Key         | 9818   | Elizabethbury      |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:15:58.097|2023-06-05 08:15:58.097|\n",
      "+------------+--------------------------------------------------------------+------------------------------+--------+--------------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- procedure_key: long (nullable = false)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: timestamp (nullable = true)\n",
      " |-- record_end_ts: timestamp (nullable = true)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: timestamp (nullable = false)\n",
      " |-- record_insert_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|procedure |procedure_key|track_hash|record_start_ts    |record_end_ts      |record_active_flag|record_upd_ts          |record_insert_ts       |\n",
      "+----------+-------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|CT Scan   |0            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:16:06.255|2023-06-05 08:16:06.255|\n",
      "|Blood Test|1            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:16:06.255|2023-06-05 08:16:06.255|\n",
      "|MRI       |2            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:16:06.255|2023-06-05 08:16:06.255|\n",
      "|ECG       |3            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:16:06.255|2023-06-05 08:16:06.255|\n",
      "|Ultrasound|4            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 08:16:06.255|2023-06-05 08:16:06.255|\n",
      "+----------+-------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for run in run3_dfs:\n",
    "    run.printSchema()\n",
    "    run.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create crawler. 618572314333:test-optimised already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler not running.\n",
      "create_table_name: optimisedlocation_dim\n",
      "create_dataset_name test-optimised-location-dim\n",
      "create_dataset_name test-optimised-location-dim\n",
      "create_dataset_name test-optimised-location-dim\n",
      "create_dataset_name test-optimised-location-dim\n",
      "create_table_name: optimisedprocedure_dim\n",
      "create_dataset_name test-optimised-procedure-dim\n",
      "create_dataset_name test-optimised-procedure-dim\n",
      "create_dataset_name test-optimised-procedure-dim\n",
      "create_dataset_name test-optimised-procedure-dim\n",
      "create_dataset_name test-optimised-procedure-dim\n",
      "Waiting for job to finish.create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      ".create_dataset_name test-optimised-procedure-dim\n",
      "\n",
      "create_dataset_name test-optimised-location-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/location_dim/full/202306051736/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-location-dim&tab=profile-overview\n",
      "create_dataset_name test-optimised-procedure-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/procedure_dim/full/202306051737/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-procedure-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "run3_paths = ['s3://test-lf-wm/etl/optimised/location_dim/full/202306051736/',\n",
    "       's3://test-lf-wm/etl/optimised/procedure_dim/full/202306051737/']\n",
    "example_path = run3_paths[-1]\n",
    "# Crawl\n",
    "glue.create_crawler(example_path)\n",
    "# glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in run3_paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)        \n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "time.sleep(10)\n",
    "\n",
    "# Print links.\n",
    "for path in run3_paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create crawler. 618572314333:test-optimised already exists\n"
     ]
    }
   ],
   "source": [
    "glue.create_crawler(['s3://test-lf-wm/etl/optimised/location_dim/full/202306051736/',\n",
    "       's3://test-lf-wm/etl/optimised/procedure_dim/full/202306051737/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://test-lf-wm/etl/optimised/location_dim/full/202306051736/\n",
      "['s', '3', ':', '/', '/', 't', 'e', 's', 't', '-', 'l', 'f', '-', 'w', 'm', '/', 'e', 't', 'l', '/', 'o', 'p', 't', 'i', 'm', 'i', 's', 'e', 'd', '/', 'l', 'o', 'c', 'a', 't', 'i', 'o', 'n', '_', 'd', 'i', 'm', '/', 'f', 'u', 'l', 'l', '/', '2', '0', '2', '3', '0', '6', '0', '5', '1', '7', '3', '6', '/']\n",
      "[{'Path': 's3://test-lf-wm/etl/optimised/location_dim'}, {'Path': 's3://test-lf-wm/etl/optimised/procedure_dim'}]\n",
      "[{'Path': 's3://test-lf-wm/etl/optimised/location_dim'}, {'Path': 's3://test-lf-wm/etl/optimised/procedure_dim'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if isinstance(paths, str):\n",
    "    path_list = [paths]\n",
    "else:\n",
    "    path_list = paths\n",
    "    \n",
    "s3_targets = [\n",
    "    {'Path':glue.create_crawler_path(path)}\n",
    "    for path \n",
    "    in path_list\n",
    "]\n",
    "print(s3_targets)\n",
    "example_path = s3_targets[0]['Path']\n",
    "name = glue.create_crawler_name(example_path)\n",
    "db_name = glue.create_db_name(example_path)\n",
    "table_prefix = glue.create_table_prefix(example_path)\n",
    "\n",
    "print(s3_targets)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stage_policyholder_into_optimised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m stage_policyholder_into_optimised\u001b[39m.\u001b[39mrun(spark,env)\n\u001b[1;32m      2\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[39m=\u001b[39m stage_provider_into_optimised\u001b[39m.\u001b[39mrun(spark,env)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stage_policyholder_into_optimised' is not defined"
     ]
    }
   ],
   "source": [
    "run4 = []\n",
    "\n",
    "df, path = stage_policyholder_into_optimised.run(spark,env)\n",
    "run4.append( (df, path) )\n",
    "df, path = stage_provider_into_optimised.run(spark,env)\n",
    "run4.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = run4_paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.create_crawler(example_path)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in run4_paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)        \n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "time.sleep(10)\n",
    "\n",
    "# Print links.\n",
    "for path in run4_paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stage_claim_into_optimised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m run5 \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m df, path \u001b[39m=\u001b[39m stage_claim_into_optimised\u001b[39m.\u001b[39mrun(spark,env)\n\u001b[1;32m      3\u001b[0m run5\u001b[39m.\u001b[39mappend( (df, path) )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stage_claim_into_optimised' is not defined"
     ]
    }
   ],
   "source": [
    "run5 = []\n",
    "df, path = stage_claim_into_optimised.run(spark,env)\n",
    "run5.append( (df, path) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

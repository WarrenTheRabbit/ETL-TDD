{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from databrew import DataBrew\n",
    "from glue import GlueWrapper\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD\")\n",
    "\n",
    "glue = GlueWrapper('us-east-1')\n",
    "databrew = DataBrew('us-east-1')\n",
    "\n",
    "# Run 1\n",
    "import stage_claim_into_raw\n",
    "import stage_policyholder_into_raw\n",
    "import stage_provider_into_raw\n",
    "\n",
    "# Run 2\n",
    "import stage_claim_into_access\n",
    "import stage_policyholder_into_access\n",
    "import stage_provider_into_access\n",
    "\n",
    "# Run 3\n",
    "import stage_location_into_optimised\n",
    "import stage_procedure_into_optimised\n",
    "\n",
    "# Run 4\n",
    "import stage_policyholder_into_optimised\n",
    "import stage_provider_into_optimised\n",
    "import stage_date_into_optimised\n",
    "\n",
    "# Run 5\n",
    "import stage_claim_into_optimised\n",
    "\n",
    "\n",
    "# import glue dependencies\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from etl.paths.components import Bucket\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_2944561/986952780.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# initialise spark session\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sc \u001b[39m=\u001b[39m SparkContext()\n\u001b[1;32m      3\u001b[0m sc\u001b[39m.\u001b[39msetLogLevel(\u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m glueContext \u001b[39m=\u001b[39m GlueContext(sc)\n",
      "File \u001b[0;32m~/spark/python/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m~/spark/python/pyspark/context.py:438\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    435\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[1;32m    437\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[1;32m    443\u001b[0m             currentAppName,\n\u001b[1;32m    444\u001b[0m             currentMaster,\n\u001b[1;32m    445\u001b[0m             callsite\u001b[39m.\u001b[39mfunction,\n\u001b[1;32m    446\u001b[0m             callsite\u001b[39m.\u001b[39mfile,\n\u001b[1;32m    447\u001b[0m             callsite\u001b[39m.\u001b[39mlinenum,\n\u001b[1;32m    448\u001b[0m         )\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_2944561/986952780.py:2 "
     ]
    }
   ],
   "source": [
    "\n",
    "# initialise spark session\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything in the raw, access and optimised tiers\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('test-lf-wm')\n",
    "\n",
    "# ********************* RESET RAW *********************\n",
    "# for obj in bucket.objects.filter(Prefix='etl/raw'):\n",
    "    # s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# **********************RESET ACCESS ******************\n",
    "# for obj in bucket.objects.filter(Prefix='etl/access'):\n",
    "    # s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# ******************** RESET OPTIMISED ****************\n",
    "for obj in bucket.objects.filter(Prefix='etl/optimised'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "raw_df, path = stage_claim_into_raw.run(spark, Bucket.TEST)\n",
    "outputs.append( (raw_df, path) )\n",
    "raw_df, path = stage_policyholder_into_raw.run(spark, Bucket.TEST)\n",
    "outputs.append( (raw_df, path) )\n",
    "raw_df, path = stage_provider_into_raw.run(spark, Bucket.TEST)\n",
    "outputs.append( (raw_df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flatten the outputs with numpy\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- claim_id: integer (nullable = true)\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- date_of_service: string (nullable = true)\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- total_procedure_cost: float (nullable = true)\n",
      " |-- medibank_pays: float (nullable = true)\n",
      " |-- medicare_pays: float (nullable = true)\n",
      " |-- excess: float (nullable = true)\n",
      " |-- out_of_pocket: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- email_address: string (nullable = true)\n",
      " |-- insurance_plan_details: string (nullable = true)\n",
      " |-- coverage_start_date: string (nullable = true)\n",
      " |-- coverage_end_date: string (nullable = true)\n",
      " |-- policy_standing: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_address: string (nullable = true)\n",
      " |-- provider_phone_number: string (nullable = true)\n",
      " |-- provider_email_address: string (nullable = true)\n",
      " |-- provider_type: string (nullable = true)\n",
      " |-- provider_license_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create crawler. 618572314333:test-raw already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler not running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset. Dataset test-raw-claim already exists.\n",
      "Couldn't create data profile job. The job test-raw-claim already exists.\n",
      "Couldn't create dataset. Dataset test-raw-provider already exists.\n",
      "Couldn't create data profile job. The job test-raw-provider already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job to finish.........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306051005/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306051005/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306051006/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(example_path)\n",
    "# glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getInstance(**kwargs={}) called\n",
      "endpoint is s3(https://s3.amazonaws.com)\n",
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "access_df, path = stage_claim_into_access.run(spark, Bucket.TEST)\n",
    "outputs.append( (access_df, path) )\n",
    "access_df, path = stage_policyholder_into_access.run(spark, Bucket.TEST)\n",
    "outputs.append( (access_df, path) )\n",
    "access_df, path = stage_provider_into_access.run(spark, Bucket.TEST)\n",
    "outputs.append( (access_df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flatten the outputs with numpy\n",
    "access_dfs = numpy.array(outputs).flatten()[::2]\n",
    "access_paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- claim_id: integer (nullable = true)\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- date_of_service: string (nullable = true)\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- total_procedure_cost: float (nullable = true)\n",
      " |-- medibank_pays: float (nullable = true)\n",
      " |-- medicare_pays: float (nullable = true)\n",
      " |-- excess: float (nullable = true)\n",
      " |-- out_of_pocket: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- policy_holder_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- email_address: string (nullable = true)\n",
      " |-- insurance_plan_details: string (nullable = true)\n",
      " |-- coverage_start_date: string (nullable = true)\n",
      " |-- coverage_end_date: string (nullable = true)\n",
      " |-- policy_standing: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- provider_id: integer (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_address: string (nullable = true)\n",
      " |-- provider_phone_number: string (nullable = true)\n",
      " |-- provider_email_address: string (nullable = true)\n",
      " |-- provider_type: string (nullable = true)\n",
      " |-- provider_license_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in access_dfs:\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish...........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/claim/full/202306051354/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/policyholder/full/202306051355/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/provider/full/202306051355/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = access_paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(example_path)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in access_paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "# Print links.\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Use data profile. #####################\n",
    "\n",
    "databrew.get_dq_results(paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run3 = []\n",
    "df, path = stage_location_into_optimised.run(spark,Bucket.TEST)\n",
    "run3.append( (df, path) )\n",
    "df, path = stage_procedure_into_optimised.run(spark,Bucket.TEST)\n",
    "run3.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['s3://test-lf-wm/etl/optimised/location_dim/full/202306052030/',\n",
       "        's3://test-lf-wm/etl/optimised/procedure_dim/full/202306052030/'],\n",
       "       dtype=object),\n",
       " array([DataFrame[location_key: bigint, address: string, street: string, postcode: string, suburb: string, track_hash: bigint, record_start_ts: string, record_end_ts: string, record_active_flag: smallint, record_upd_ts: string, record_insert_ts: string],\n",
       "        DataFrame[procedure: string, procedure_key: bigint, track_hash: bigint, record_start_ts: string, record_end_ts: string, record_active_flag: smallint, record_upd_ts: string, record_insert_ts: string]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run3_dfs = numpy.array(run3).flatten()[0::2]\n",
    "run3_paths = numpy.array(run3).flatten()[1::2]\n",
    "\n",
    "run3_paths, run3_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_key: long (nullable = false)\n",
      " |-- address: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- suburb: string (nullable = true)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------------------------------------+------------------------------+--------+--------------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|location_key|address                                                       |street                        |postcode|suburb              |track_hash|record_start_ts    |record_end_ts      |record_active_flag|record_upd_ts          |record_insert_ts       |\n",
      "+------------+--------------------------------------------------------------+------------------------------+--------+--------------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|0           |Level 2 85 Rhonda Upper, Jimmyfurt, TAS, 2987                 |Level 2 85 Rhonda Upper       | 2987   | Jimmyfurt          |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 10:31:23.225|2023-06-05 10:31:23.225|\n",
      "|1           |Apt. 486 646 Michael Concourse, Lake Gabrielchester, VIC, 2901|Apt. 486 646 Michael Concourse| 2901   | Lake Gabrielchester|null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 10:31:23.225|2023-06-05 10:31:23.225|\n",
      "+------------+--------------------------------------------------------------+------------------------------+--------+--------------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- procedure: string (nullable = true)\n",
      " |-- procedure_key: long (nullable = false)\n",
      " |-- track_hash: long (nullable = true)\n",
      " |-- record_start_ts: string (nullable = false)\n",
      " |-- record_end_ts: string (nullable = false)\n",
      " |-- record_active_flag: short (nullable = false)\n",
      " |-- record_upd_ts: string (nullable = false)\n",
      " |-- record_insert_ts: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|procedure |procedure_key|track_hash|record_start_ts    |record_end_ts      |record_active_flag|record_upd_ts          |record_insert_ts       |\n",
      "+----------+-------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "|CT Scan   |0            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 10:31:25.852|2023-06-05 10:31:25.852|\n",
      "|Blood Test|1            |null      |1970-01-01 00:00:00|2999-12-31 00:00:00|1                 |2023-06-05 10:31:25.852|2023-06-05 10:31:25.852|\n",
      "+----------+-------------+----------+-------------------+-------------------+------------------+-----------------------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for run in run3_dfs:\n",
    "    run.printSchema()\n",
    "    run.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't delete crawler. Crawler entry with name test-optimised does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for crawler to finish...........\n",
      "create_table_name(): optimisedlocation_dim\n",
      "create_dataset_name(): test-optimised-location-dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-location-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_bucket_name(): test-lf-wm\n",
      "create_dataset_name(): test-optimised-location-dim\n",
      "create_job_name(): test-optimised-location-dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create data profile job:\n",
      "                The job test-optimised-location-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_job_name(): test-optimised-location-dim\n",
      "create_table_name(): optimisedprocedure_dim\n",
      "create_dataset_name(): test-optimised-procedure-dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-procedure-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_bucket_name(): test-lf-wm\n",
      "create_dataset_name(): test-optimised-procedure-dim\n",
      "create_job_name(): test-optimised-procedure-dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create data profile job:\n",
      "                The job test-optimised-procedure-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_job_name(): test-optimised-procedure-dim\n",
      "create_job_name(): test-optimised-procedure-dim\n",
      "Waiting for job to finish.create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      ".create_job_name(): test-optimised-procedure-dim\n",
      "\n",
      "create_dataset_name(): test-optimised-location-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/location_dim/full/202306052030/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-location-dim&tab=profile-overview\n",
      "create_dataset_name(): test-optimised-procedure-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/procedure_dim/full/202306052030/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-procedure-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = run3_paths[-1]\n",
    "# Crawl\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(run3_paths)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in run3_paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)        \n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "time.sleep(10)\n",
    "\n",
    "# Print links.\n",
    "for path in run3_paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "module DataBrew not in sys.modules",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# reload stage_date_into_optimised\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m importlib\u001b[39m.\u001b[39;49mreload(DataBrew)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/importlib/__init__.py:148\u001b[0m, in \u001b[0;36mreload\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mget(name) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m module:\n\u001b[1;32m    147\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not in sys.modules\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(name), name\u001b[39m=\u001b[39mname)\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _RELOADING:\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m _RELOADING[name]\n",
      "\u001b[0;31mImportError\u001b[0m: module DataBrew not in sys.modules"
     ]
    }
   ],
   "source": [
    "# reload stage_date_into_optimised\n",
    "import importlib\n",
    "importlib.reload(DataBrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/location_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/location_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run4 = []\n",
    "\n",
    "env=Bucket.TEST\n",
    "\n",
    "df, path = stage_policyholder_into_optimised.run(spark,env)\n",
    "run4.append( (df, path) )\n",
    "df, path = stage_provider_into_optimised.run(spark,env)\n",
    "run4.append( (df, path) )\n",
    "df, path = stage_date_into_optimised.run(spark,env)\n",
    "run4.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "run4_dfs = numpy.array(run4).flatten()[0::2]\n",
    "run4_paths = numpy.array(run4).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't delete crawler. Crawler entry with name test-optimised does not exist\n"
     ]
    }
   ],
   "source": [
    "glue.delete_crawler(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(glue_databrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for crawler to finish..........\n",
      "create_table_name(): optimisedpolicyholder_dim\n",
      "create_dataset_name(): test-optimised-policyholder-dim\n",
      "get_bucket_name(): test-lf-wm\n",
      "create_dataset_name(): test-optimised-policyholder-dim\n",
      "create_job_name(): test-optimised-policyholder-dim\n",
      "create_job_name(): test-optimised-policyholder-dim\n",
      "create_table_name(): optimisedprovider_dim\n",
      "create_dataset_name(): test-optimised-provider-dim\n",
      "get_bucket_name(): test-lf-wm\n",
      "create_dataset_name(): test-optimised-provider-dim\n",
      "create_job_name(): test-optimised-provider-dim\n",
      "create_job_name(): test-optimised-provider-dim\n",
      "create_table_name(): optimiseddate_dim\n",
      "create_dataset_name(): test-optimised-date-dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create dataset:\n",
      "                Dataset test-optimised-date-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_bucket_name(): test-lf-wm\n",
      "create_dataset_name(): test-optimised-date-dim\n",
      "create_job_name(): test-optimised-date-dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't create data profile job:\n",
      "                The job test-optimised-date-dim already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_job_name(): test-optimised-date-dim\n",
      "create_job_name(): test-optimised-date-dim\n",
      "Waiting for job to finish.create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      ".create_job_name(): test-optimised-date-dim\n",
      "\n",
      "create_dataset_name(): test-optimised-policyholder-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/policyholder_dim/full/202306052035/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-policyholder-dim&tab=profile-overview\n",
      "create_dataset_name(): test-optimised-provider-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/provider_dim/full/202306052035/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-provider-dim&tab=profile-overview\n",
      "create_dataset_name(): test-optimised-date-dim\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/date_dim/full/202306052036/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-date-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_path = run4_paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(run4_paths)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in run4_paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)        \n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "# Print links.\n",
    "for path in run4_paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs is {'region_name': 'us-east-1'}\n",
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/date_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/policyholder_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/procedure_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/provider_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = glueContext.spark_session\n",
    "env = Bucket.TEST\n",
    "\n",
    "run5 = []\n",
    "df, path = stage_claim_into_optimised.run(spark,env)\n",
    "run5.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run5_dfs = numpy.array(run5).flatten()[0::2]\n",
    "run5_paths = numpy.array(run5).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- claim_id: integer (nullable = true)\n",
      " |-- provider_key: long (nullable = true)\n",
      " |-- policyholder_key: long (nullable = true)\n",
      " |-- procedure_key: long (nullable = true)\n",
      " |-- date_key: long (nullable = true)\n",
      " |-- total_procedure_cost: float (nullable = true)\n",
      " |-- medibank_pays: float (nullable = true)\n",
      " |-- medicare_pays: float (nullable = true)\n",
      " |-- excess: float (nullable = true)\n",
      " |-- out_of_pocket: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run5_dfs[0].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish.........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/claim_fact/full/202306052051/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-claim-fact&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_path = run5_paths[-1]\n",
    "\n",
    "# Crawl\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(run5_paths)\n",
    "glue.start_crawler(example_path)\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "for path in run5_paths:\n",
    "     \n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)        \n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "# Print links.\n",
    "for path in run5_paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Redshift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "# Import glue dependencies.\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# Add paths so that this notebook can be executed anywhere in the \n",
    "# workspace's folder structure.\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD\")\n",
    "sys.path.append(\"/home/glue_user/project_lf/ETL-TDD/automation\")\n",
    "\n",
    "# Import batch job's custom dependencies.\n",
    "from databrew import DataBrew\n",
    "from glue import GlueWrapper\n",
    "from etl.paths.components import Bucket\n",
    "\n",
    "# Import jobs.\n",
    "import stage_claim_into_raw\n",
    "import stage_policyholder_into_raw\n",
    "import stage_provider_into_raw\n",
    "import stage_claim_into_access\n",
    "import stage_policyholder_into_access\n",
    "import stage_provider_into_access\n",
    "import stage_location_into_optimised\n",
    "import stage_procedure_into_optimised\n",
    "import stage_policyholder_into_optimised\n",
    "import stage_provider_into_optimised\n",
    "import stage_date_into_optimised\n",
    "import stage_claim_into_optimised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialise spark session with minimal logging.\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Wrap low-level AWS clients in a high-level object oriented API that uses\n",
    "# S3 paths to create and coordinate AWS services.\n",
    "glue = GlueWrapper('us-east-1')\n",
    "databrew = DataBrew('us-east-1')\n",
    "\n",
    "# Define the bucket to use (TEST or PROD).\n",
    "env = Bucket.TEST\n",
    "\n",
    "# Define the sequential job batches.\n",
    "# Earlier batches must be completed before later batches.\n",
    "batch1 = [ stage_claim_into_raw, \n",
    "           stage_policyholder_into_raw, \n",
    "           stage_provider_into_raw ]\n",
    "\n",
    "batch2 = [ stage_claim_into_access, \n",
    "           stage_policyholder_into_access, \n",
    "           stage_provider_into_access ]\n",
    "\n",
    "batch3 = [ stage_location_into_optimised, \n",
    "           stage_procedure_into_optimised ]\n",
    "\n",
    "batch4 = [ stage_policyholder_into_optimised, \n",
    "           stage_provider_into_optimised, \n",
    "           stage_date_into_optimised ]\n",
    "\n",
    "batch5 = [ stage_claim_into_optimised ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete everything in the raw, access and optimised tiers\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('test-lf-wm')\n",
    "\n",
    "# ********************* RESET RAW *********************\n",
    "for obj in bucket.objects.filter(Prefix='etl/raw'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# **********************RESET ACCESS ******************\n",
    "for obj in bucket.objects.filter(Prefix='etl/access'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "\n",
    "# ******************** RESET OPTIMISED ****************\n",
    "for obj in bucket.objects.filter(Prefix='etl/optimised'):\n",
    "    s3.Object(bucket.name,obj.key).delete()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch1:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-raw.\n",
      "Waiting for crawler to finish.........................\n",
      "Waiting for job to finish....................\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/claim/full/202306052301/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/policyholder/full/202306052301/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/raw/claim_db/provider/full/202306052302/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-raw-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/raw/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch2:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-access.\n",
      "Waiting for crawler to finish............\n",
      "Waiting for job to finish........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/claim/full/202306052307/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-claim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/policyholder/full/202306052307/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-policyholder&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/access/claim_db/provider/full/202306052308/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-access-provider&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Use data profile. #####################\n",
    "\n",
    "# databrew.get_dq_results(paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch3:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't delete crawler:\n",
      "                Crawler entry with name test-optimised does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-optimised.\n",
      "Waiting for crawler to finish..........\n",
      "Waiting for job to finish.........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/location_dim/full/202306052312/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-location-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/procedure_dim/full/202306052313/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-procedure-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/policyholder/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/location_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/provider/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/location_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/glue_user/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch4:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-optimised.\n",
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish........................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/policyholder_dim/full/202306052317/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-policyholder-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/provider_dim/full/202306052317/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-provider-dim&tab=profile-overview\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/date_dim/full/202306052318/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-date-dim&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "################# Delete outdated crawler #################\n",
    "glue.delete_crawler(example_path)\n",
    "\n",
    "################# Crawl all batch paths. ##################\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "################### Profile each job. #####################\n",
    "for path in paths:\n",
    "     \n",
    "    response = databrew.create_dataset(path)\n",
    "    response = databrew.create_profile_job(path)        \n",
    "    response = databrew.start_job_run(path)\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "############## Print data profile links ###################\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/access/claim_db/claim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/date_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/policyholder_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/procedure_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='test-lf-wm') etl/optimised/provider_dim/full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for job in batch5:\n",
    "    df, path = job.run(spark, env)\n",
    "    outputs.append( (df, path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all job paths and a list of all job dataframes.\n",
    "dfs = numpy.array(outputs).flatten()[::2]\n",
    "paths = numpy.array(outputs).flatten()[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler test-optimised.\n",
      "Waiting for crawler to finish...........\n",
      "Waiting for job to finish.......................\n",
      "You can view the data profile for s3://test-lf-wm/etl/optimised/claim_fact/full/202306052322/ here:\n",
      "\thttps://us-east-1.console.aws.amazon.com/databrew/home?region=us-east-1#dataset-details?dataset=test-optimised-claim-fact&tab=profile-overview\n"
     ]
    }
   ],
   "source": [
    "example_path = paths[-1]\n",
    "\n",
    "# Crawl each job path as an S3 target.\n",
    "glue.delete_crawler(example_path)\n",
    "glue.create_crawler(paths)\n",
    "glue.start_crawler(example_path)\n",
    "\n",
    "############## Wait for crawler to finish #####################\n",
    "\n",
    "glue.wait_for_crawler(example_path)\n",
    "\n",
    "############## After crawlers finish. #####################\n",
    "\n",
    "for path in paths:\n",
    "    databrew.create_dataset(path)\n",
    "    databrew.create_profile_job(path)\n",
    "    databrew.start_job_run(path)\n",
    "\n",
    "############## Wait for job to finish #####################\n",
    "\n",
    "databrew.wait_for_job(example_path)\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"You can view the data profile for {path} here:\\n\\t{databrew.get_profile_link(path)}\")\n",
    "    # databrew.show_data_profile_link(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift - Initial Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = redshift_connector.connect(\n",
    "    host='default.618572314333.ap-southeast-2.redshift-serverless.amazonaws.com:5439/dev',\n",
    "    port=5439,\n",
    "    database='dev',\n",
    "    user='awsuser',\n",
    "    password='my_password'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "('communication error', gaierror(-2, 'Name or service not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/redshift_connector/core.py:596\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, user, password, database, host, port, source_address, unix_sock, ssl, sslmode, timeout, max_prepared_statements, tcp_keepalive, application_name, replication, client_protocol_version, database_metadata_current_db_only, credentials_provider, provider_name, web_identity_token, numeric_to_float)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m unix_sock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_usock\u001b[39m.\u001b[39;49mconnect((host, port))\n\u001b[1;32m    597\u001b[0m \u001b[39melif\u001b[39;00m unix_sock \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Connect to the cluster\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mredshift_connector\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m conn \u001b[39m=\u001b[39m redshift_connector\u001b[39m.\u001b[39;49mconnect(\n\u001b[1;32m      4\u001b[0m     host\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexamplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     database\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdev\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     port\u001b[39m=\u001b[39;49m\u001b[39m5439\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     user\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mawsuser\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmy_password\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39m# Create a Cursor object\u001b[39;00m\n\u001b[1;32m     12\u001b[0m cursor \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/redshift_connector/__init__.py:344\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(user, database, password, port, host, source_address, unix_sock, ssl, sslmode, timeout, max_prepared_statements, tcp_keepalive, application_name, replication, idp_host, db_user, app_id, app_name, preferred_role, principal_arn, access_key_id, secret_access_key, session_token, profile, credentials_provider, region, cluster_identifier, iam, client_id, idp_tenant, client_secret, partner_sp_id, idp_response_timeout, listen_port, login_url, auto_create, db_groups, force_lowercase, allow_db_user_override, client_protocol_version, database_metadata_current_db_only, ssl_insecure, web_identity_token, role_session_name, role_arn, iam_disable_cache, auth_profile, endpoint_url, provider_name, scope, numeric_to_float, is_serverless, serverless_acct_id, serverless_work_group, group_federation)\u001b[0m\n\u001b[1;32m    341\u001b[0m _logger\u001b[39m.\u001b[39mdebug(mask_secure_info_in_props(info)\u001b[39m.\u001b[39m\u001b[39m__str__\u001b[39m())\n\u001b[1;32m    342\u001b[0m _logger\u001b[39m.\u001b[39mdebug(make_divider_block())\n\u001b[0;32m--> 344\u001b[0m \u001b[39mreturn\u001b[39;00m Connection(\n\u001b[1;32m    345\u001b[0m     user\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49muser_name,\n\u001b[1;32m    346\u001b[0m     host\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mhost,\n\u001b[1;32m    347\u001b[0m     database\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mdb_name,\n\u001b[1;32m    348\u001b[0m     port\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mport,\n\u001b[1;32m    349\u001b[0m     password\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mpassword,\n\u001b[1;32m    350\u001b[0m     source_address\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    351\u001b[0m     unix_sock\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49munix_sock,\n\u001b[1;32m    352\u001b[0m     ssl\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mssl,\n\u001b[1;32m    353\u001b[0m     sslmode\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49msslmode,\n\u001b[1;32m    354\u001b[0m     timeout\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    355\u001b[0m     max_prepared_statements\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mmax_prepared_statements,\n\u001b[1;32m    356\u001b[0m     tcp_keepalive\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mtcp_keepalive,\n\u001b[1;32m    357\u001b[0m     application_name\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mapplication_name,\n\u001b[1;32m    358\u001b[0m     replication\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mreplication,\n\u001b[1;32m    359\u001b[0m     client_protocol_version\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mclient_protocol_version,\n\u001b[1;32m    360\u001b[0m     database_metadata_current_db_only\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mdatabase_metadata_current_db_only,\n\u001b[1;32m    361\u001b[0m     credentials_provider\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mcredentials_provider,\n\u001b[1;32m    362\u001b[0m     provider_name\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mprovider_name,\n\u001b[1;32m    363\u001b[0m     web_identity_token\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mweb_identity_token,\n\u001b[1;32m    364\u001b[0m     numeric_to_float\u001b[39m=\u001b[39;49minfo\u001b[39m.\u001b[39;49mnumeric_to_float,\n\u001b[1;32m    365\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/redshift_connector/core.py:643\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, user, password, database, host, port, source_address, unix_sock, ssl, sslmode, timeout, max_prepared_statements, tcp_keepalive, application_name, replication, client_protocol_version, database_metadata_current_db_only, credentials_provider, provider_name, web_identity_token, numeric_to_float)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_usock\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 643\u001b[0m     \u001b[39mraise\u001b[39;00m InterfaceError(\u001b[39m\"\u001b[39m\u001b[39mcommunication error\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush: typing\u001b[39m.\u001b[39mCallable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mflush\n\u001b[1;32m    645\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read: typing\u001b[39m.\u001b[39mCallable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mread\n",
      "\u001b[0;31mInterfaceError\u001b[0m: ('communication error', gaierror(-2, 'Name or service not known'))"
     ]
    }
   ],
   "source": [
    "#Connect to the cluster\n",
    "import redshift_connector\n",
    "conn = redshift_connector.connect(\n",
    "    host='examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com',\n",
    "    database='dev',\n",
    "    port=5439,\n",
    "    user='awsuser',\n",
    "    password='my_password'\n",
    ")\n",
    "\n",
    "# Create a Cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query a table using the Cursor\n",
    "cursor.execute(\"select * from book\")\n",
    "            \n",
    "#Retrieve the query result set\n",
    "result: tuple = cursor.fetchall()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
